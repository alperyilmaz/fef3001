[
  {
    "objectID": "ders1.html",
    "href": "ders1.html",
    "title": "GPT models",
    "section": "",
    "text": "GPT1, GPT2 abd GPT3 were released couple years before ChatGPT and it was able to complete sentences. Please visit “How Does ChatGPT Work?” site for more information. After ChatGPT was released, large language models got “conversational”\nPlease visit OpenAI Playground completion site and select “gpt-3.5-turbo” as model. Type a incomplete sentence and then press Submit button.\nYou’ll see that the model will complete your sentence. If you turn on the “Show probabilities” option on right menu, you’ll get a glimpse of how GPT works.\nIf you complete the sentence and then hover over the words, you’ll see that the model had many words to chose from with certain probabilities. In this example, “my” is the 4th most probable word after Today is and “birthday” is most probable word after Today is my.\nOne of key settings of GPT models is temperature. If you decrease the temperature the model will choose the most probably words. If temperature is high, the model will be picking words with low probabalities, which will bring creative and diverse output.\nIf you decrease the temperature to zero, the output will be always same."
  },
  {
    "objectID": "ders1.html#chatgpt-writes-code-for-us",
    "href": "ders1.html#chatgpt-writes-code-for-us",
    "title": "GPT models",
    "section": "ChatGPT writes code for us",
    "text": "ChatGPT writes code for us\nLet’s ask ChatGPT for a fast prime number function in Python\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nDid you see what just happened! It provided code tailored for us and it also explained bits of the code. Have you noticed that the code contains the tricks we have learned during our lecture: * early stop: if a condition is met, return immediately True or False * instead of checking until N, we should check numbers until \\(\\sqrt{N}\\) (n  0.5** in Python)\nYou can continue the conversation. Let’s ask for memoization.\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nLet’s get the code and test it here:\n\ndef is_prime(n, memo={}):\n    if n in memo:\n        return memo[n]\n    if n in (2, 3):\n        result = True\n    elif n == 1 or n % 2 == 0:\n        result = False\n    else:\n        result = True\n        for i in range(3, int(n ** 0.5) + 1, 2):\n            if n % i == 0:\n                result = False\n                break\n    memo[n] = result\n    return result\n\n\n# is_prime(100000000003)\n\n\n&lt;i class=\"fas fa-fw fa-exclamation-circle mr-3 align-self-center\"&gt;&lt;/i&gt;\n&lt;b&gt;Warning:&lt;/b&gt; At the time of writing this note (January 2023) chatGPT is known to provide results which are not exactly true!&lt;br&gt;So, be aware and don't use code or information you got from chatGPT as is without checking or confirming."
  },
  {
    "objectID": "ders1.html#chatgpt-can-fix-or-modify-the-code",
    "href": "ders1.html#chatgpt-can-fix-or-modify-the-code",
    "title": "GPT models",
    "section": "ChatGPT can fix or modify the code",
    "text": "ChatGPT can fix or modify the code\nYou have a piece of code and it does not work as intended? Then you can ask ChatGPT to fix the code.\nLet’s provide a code with error and ask chatGPT to debug it:\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nSo, ChatGPT can fix code! By the way, did you know that I used ChatGPT to prepare that example ;)\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "ders1.html#is-it-the-end-of-programming",
    "href": "ders1.html#is-it-the-end-of-programming",
    "title": "GPT models",
    "section": "Is it the end of programming?",
    "text": "Is it the end of programming?\nYou might think, “we learned Python for a semester and but now there’s ChatGPT, no need to learn coding at all!”. You might find content online which are exploring similar concerns. But more sane approach is “AI tools will increase speed and efficiency of developers also will decresase barriers to entry for new languages.\nThe section below is taken from Github Copilot’s page:"
  },
  {
    "objectID": "ders1.html#thanks-for-showing-us-chatgpt-i-can-now-prepare-my-homeworks-easily",
    "href": "ders1.html#thanks-for-showing-us-chatgpt-i-can-now-prepare-my-homeworks-easily",
    "title": "GPT models",
    "section": "Thanks for showing us ChatGPT, I can now prepare my homeworks easily!",
    "text": "Thanks for showing us ChatGPT, I can now prepare my homeworks easily!\nSeeing the capabilities of ChatGPT might give some hint about misuse. First of all, a homework is for you to learn a concept (remember the playing the guitar analogy?) so being involved in AI-assisted plagiarism is still a plagiarism and a loss on your side. (please also check the file 00-academic-integrity.ipynb for adverse effects of plagiarism)\nSecond of all, OpenAI and others are working on tools and ways to include watermarks in ChatGPT output so that it can be spotted easily.\nSo, please be inspired by this tool, use it to augment your learning but do not use it for plagiarism."
  },
  {
    "objectID": "ders1.html#closed-and-open-models",
    "href": "ders1.html#closed-and-open-models",
    "title": "GPT models",
    "section": "Closed and Open models",
    "text": "Closed and Open models\nCurrently there are several closed-source large language models which are developed by major corparations each took hundreds of millions to train * GPT3.5 (ChatGPT) and GPT4 by OpenAI * PaLM and Bard by Google * Claude by Anthropics"
  },
  {
    "objectID": "ders1.html#claude-3.5-sonnet-by-antropic",
    "href": "ders1.html#claude-3.5-sonnet-by-antropic",
    "title": "GPT models",
    "section": "Claude 3.5 Sonnet by Antropic",
    "text": "Claude 3.5 Sonnet by Antropic\nClaude 3.5 is the latest version of Claude model. Antropic releases Claude in three sizes Haiku (smallest), Sonnet (medium) and Opus (largest). Currently, Claude 3.5 Sonnest is the best LLM. Generally Claude is good at creative writing but with the latest release it excels at coding as well. Also, Claude allows preview of the code (web-based) on right panel.\nBelow is as example where Claude 3.5 Sonnet was prompted “Please write game of snake in HTML, CSS and JS”. You can actually play the game!\n\nClaude also allows uploading documents (e.g. PDF) and asking questions about the document.\nYou can use Claude (with some limitations) at https://claude.ai/chat after registering an account for free."
  },
  {
    "objectID": "ders1.html#gemini-by-google",
    "href": "ders1.html#gemini-by-google",
    "title": "GPT models",
    "section": "Gemini by Google",
    "text": "Gemini by Google\nGemini is actually an LLM with online search capacity. Also, you can download documents (via Google Drive) or point to YouTube videos to ask questions. Finally, Gemini has the longest context window of 1 million tokens (will fit around 10 books at once)."
  },
  {
    "objectID": "ders1.html#running-models-locally",
    "href": "ders1.html#running-models-locally",
    "title": "GPT models",
    "section": "Running models locally",
    "text": "Running models locally\nSince developers share the model weights, it’s possible to download and run the models locally. There are various sizes of models. 7B (7 billion) models require around 4Gb memory, so you can run them in your laptop.\nYou can run the models by installing PyTorch and some other libraries for Python and then writing some Python code. Or you can install Ollama and then run any compatible model with it.\nAdvantages of running a model locally: 1. Less Censorship 2. Better Data Privacy 3. Offline Usage 4. Cost Savings 5. Better Customization\nDisadvantages of running a model locally: 1. Resource Intensive 2. Slower Responses and Inferior Performance 3. Complex Setup\n\nRunning a model locally using Jupyter notebook or Google Colab\nYou can actually run LLM models in Jupyter notebooks. However the process will require you to install lots of Python packages, downloading the model weights and then writing some Python code to ask questions to the model and then capturing the answer in a dictionary. Although there are some solutions for “chat-like” experience, the experince is more like “one-shot question and answer”.\nPlease check the Youtube video which describes the process in Google Colab, where Google provides GPU (or TPU) to run the model in a Jupyter notebook environment.\n\n\nRunning a model locally with Ollama\n\nPlease visit the list of models page to have an idea about specialized models. With Ollama, you can download and use any of those models.\nIn the terminal, let’s list available models\n$ ollama list\n\nNAME                ID              SIZE    MODIFIED\ndeepseek-coder:6.7b 72be2442d736    3.8 GB  5 weeks ago     \nneural-chat:latest  73940af9fe02    4.1 GB  5 weeks ago     \norca2:7b            ea98cc422de3    3.8 GB  5 weeks ago     \nphi:latest          c651b7a89d73    1.6 GB  14 hours ago    \nsolar:latest        059fdabbe6e6    6.1 GB  2 hours ago     \nstablelm-zephyr:3b  7c596e78b1fc    1.6 GB  3 weeks ago\nLet’s run Phi-2 by Microsoft. Here’s info about Phi-2:\n\na 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.\n\n$ ollama run phi:latest\n\n&gt;&gt;&gt; why sky is blue?\n\n The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere,\nit collides with molecules and tiny particles in the air, such as oxygen and nitrogen atoms. These collisions cause\nthe shorter wavelengths of light (blue) to scatter more than the longer wavelengths (red, orange, yellow, green, and\nviolet). As a result, our eyes perceive the scattered blue light to be dominant, which is why the sky appears blue to us.\nAs you can see, a small model, which can answer questions, help coding can be run locally.\n\n\nRunning a model locally with user interface\n\nOllama Web UI\n\n\n\nLLM Studio\nYou can install LM Studio and then interact with local models with its user interface"
  },
  {
    "objectID": "ders1.html#github-copilot",
    "href": "ders1.html#github-copilot",
    "title": "GPT models",
    "section": "Github CoPilot",
    "text": "Github CoPilot\n\n%%HTML\n&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ZapdeEJ7xJw?si=3DYWvMa7uudDhUnj\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "ders1.html#free-github-copilot-alternative",
    "href": "ders1.html#free-github-copilot-alternative",
    "title": "GPT models",
    "section": "Free Github CoPilot alternative",
    "text": "Free Github CoPilot alternative\nCody is Sourcegraph’s AI coding assistant, and it has a couple of features that no other assistant has to make you a 10x developer. You can check it out here: https://sourcegraph.com/cody"
  },
  {
    "objectID": "ders1.html#open-source-local-alternatives",
    "href": "ders1.html#open-source-local-alternatives",
    "title": "GPT models",
    "section": "Open source local alternatives",
    "text": "Open source local alternatives\nPlease visit https://tabby.tabbyml.com/ for more information"
  },
  {
    "objectID": "ders1.html#image-generation",
    "href": "ders1.html#image-generation",
    "title": "GPT models",
    "section": "Image generation",
    "text": "Image generation\n\nDALL-E 3 by OpenAI\nPrompt: “a dna strand with istanbul city as silhoutte in background”\nRevised prompt: “Imagine a detailed science meets architecture illustration. In the foreground, there’s a strand of DNA, magnified and glowing against a dark backdrop. This helix structure is precisely depicted, its familiar double-helix shape twisting majestically. Intertwining with this scientific image, in the background, there’s a silhouette representation of Istanbul city. The skyline shows off its marvelous historical architecture, including its domed mosques and towering minarets. The cityscape sits against a dusky eveningsky, the setting sun casting an orange glow on the horizon behind the buildings. The combination results in a fascinating juxtapositionof science and urban beauty.”\nResult:\n\n\n\nMidjourney v6\nMidjourney announced v6 in December 2023. It can generate photorealistic images. Normally, you need to use their Discord channel to generate images free. Also, you can generate images using getimg.ai. I tried generating images using the following prompts\n\nMolecular biology lab with lots of students.\nBeautiful sea side with white sand beach and tropical trees on shore. The sea looks greenish blue\n\nThe results are great:\n\n\n\nDisinformation\nBe aware of deepfake or AI-generated fake photos\nFake Trump Photo\n\nFake Pope Photo"
  },
  {
    "objectID": "ders1.html#video-generation",
    "href": "ders1.html#video-generation",
    "title": "GPT models",
    "section": "Video generation",
    "text": "Video generation\n\nPrompt to video\n\nRunwayML Gen-2\nPika\nStable Video Diffusion by Stable Diffusion\nGENMO\n\n\n\nImage to video\n\nGENMO"
  },
  {
    "objectID": "ders1.html#music-or-sound-generation",
    "href": "ders1.html#music-or-sound-generation",
    "title": "GPT models",
    "section": "Music or sound generation",
    "text": "Music or sound generation\n\nElevenlabs can convert text to speech online for free with our AI voice generator"
  },
  {
    "objectID": "ders10-regression.html#ml-types",
    "href": "ders10-regression.html#ml-types",
    "title": "Regression Methods",
    "section": "ML types",
    "text": "ML types\n\nClassical ML\n\nSupervised\n\nClassification\nRegression\n\nUnsupervised\n\nClustering"
  },
  {
    "objectID": "ders10-regression.html#contents",
    "href": "ders10-regression.html#contents",
    "title": "Regression Methods",
    "section": "Contents",
    "text": "Contents\n\nDefinition of regression\nTypes of regression (linear, polynomial, and ensemble methods)\n\nLinear Regression: Simple Linear Regression, Ordinary Least Squares method, Evaluating model performance (R-squared, RMSE)\nPolynomial Regression: When to use polynomial regression, overfitting\nRegression with Random Forest: Random Forest algorithm for regression, Advantages (handling non-linearity, feature importance)\nOther types of regression: Logistic regression (for binary outcomes), Ridge and Lasso regression (regularization techniques)\n\nExamples on Excel\nCode examples in R and Python"
  },
  {
    "objectID": "ders10-regression.html#regression",
    "href": "ders10-regression.html#regression",
    "title": "Regression Methods",
    "section": "Regression",
    "text": "Regression\nRegression is a statistical method used in data analysis and machine learning to model and analyze the relationship between a dependent variable (often called the target or outcome variable) and one or more independent variables (also known as predictors or features). The primary goal of regression is to estimate how changes in the independent variables are associated with changes in the dependent variable, allowing for prediction and inference.\nKey points about regression:\n\nIt helps identify and quantify relationships between variables.\nIt can be used for both prediction and understanding the impact of variables.\nThere are various types of regression, including linear, polynomial, and more complex methods like Random Forest or Neural networks for regression.\nRegression models can handle both continuous and categorical variables.\nIt’s widely used in many fields, including science, economics, and social sciences."
  },
  {
    "objectID": "ders10-regression.html#linear-regression",
    "href": "ders10-regression.html#linear-regression",
    "title": "Regression Methods",
    "section": "Linear Regression",
    "text": "Linear Regression"
  },
  {
    "objectID": "ders10-regression.html#linear-regression-1",
    "href": "ders10-regression.html#linear-regression-1",
    "title": "Regression Methods",
    "section": "Linear Regression",
    "text": "Linear Regression"
  },
  {
    "objectID": "ders10-regression.html#linear-regression-2",
    "href": "ders10-regression.html#linear-regression-2",
    "title": "Regression Methods",
    "section": "Linear Regression",
    "text": "Linear Regression\ntry finding the best line: https://www.geogebra.org/m/xC6zq7Zv"
  },
  {
    "objectID": "ders10-regression.html#ols-method",
    "href": "ders10-regression.html#ols-method",
    "title": "Regression Methods",
    "section": "OLS method",
    "text": "OLS method\nOrdinary Least Squares (OLS) is a statistical method used to estimate the parameters of a linear regression model. It’s the most common technique for fitting a line to a set of data points in linear regression.\nOLS is a method that minimizes the sum of the squared differences between the observed dependent variable values and the predicted values by the linear function of the independent variable(s). In other words, it finds the line (or hyperplane in multiple dimensions) that best fits the data by minimizing the sum of the squared residuals.\nPlease visit OLS Method at Wikipedia for technical details."
  },
  {
    "objectID": "ders10-regression.html#multivariate-linear-regression",
    "href": "ders10-regression.html#multivariate-linear-regression",
    "title": "Regression Methods",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nMultivariate Linear Regression, also known as Multiple Linear Regression, is a statistical method used to model the relationship between multiple independent variables and a single dependent variable. It extends the concept of simple linear regression to include more than one predictor variable.\nThe general form of the model is:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\varepsilon\n\\]\nWhere:\ny is the dependent variable, x₁, x₂, …, xₙ are the independent variables, β₀ is the y-intercept (the value of y when all x’s are zero), β₁, β₂, …, βₙ are the coefficients associated with each independent variable, n is the number of independent variables, ε is the error term"
  },
  {
    "objectID": "ders10-regression.html#polynomial-regression",
    "href": "ders10-regression.html#polynomial-regression",
    "title": "Regression Methods",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nPolynomial Regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. It’s used when the relationship between variables is not linear but can be approximated by a polynomial function.\n\\[\ny = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\ldots + \\beta_nx^n + \\varepsilon\n\\]\nWhere:\ny is the dependent variable, x is the independent variable, β₀, β₁, β₂, …, βₙ are the coefficients to be estimated, n is the degree of the polynomial and ε is the error term"
  },
  {
    "objectID": "ders10-regression.html#polynomial-regression-1",
    "href": "ders10-regression.html#polynomial-regression-1",
    "title": "Regression Methods",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression"
  },
  {
    "objectID": "ders10-regression.html#linear-regression-in-excel",
    "href": "ders10-regression.html#linear-regression-in-excel",
    "title": "Regression Methods",
    "section": "Linear Regression in Excel",
    "text": "Linear Regression in Excel\nUse MTCARS dataset Excel file and;\n\ndraw Trendline between wt (weight) and mpg (miles per galon) columns.\ndraw Trendline between disp (displacement) and wt (weight) columns.\n\nPlease visit MyExcelOnline in order see how to draw a Trendline in Excel"
  },
  {
    "objectID": "ders10-regression.html#linear-regression-in-r",
    "href": "ders10-regression.html#linear-regression-in-r",
    "title": "Regression Methods",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\nPlease refer to Linear Regression with R page for simple linear and polynomial regression."
  },
  {
    "objectID": "ders10-regression.html#regression-with-random-forest",
    "href": "ders10-regression.html#regression-with-random-forest",
    "title": "Regression Methods",
    "section": "Regression with Random Forest",
    "text": "Regression with Random Forest\nPlease refer to random forest regression with R document."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html",
    "href": "An_Introduction_to_Hierarchical_Clustering.html",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "",
    "text": "Taken from the article “An Introduction to Hierarchical Clustering in Python” from DataCamp"
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#definition-of-hierarchical-clustering",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#definition-of-hierarchical-clustering",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "Definition of Hierarchical Clustering",
    "text": "Definition of Hierarchical Clustering\nA hierarchical clustering approach is based on the determination of successive clusters based on previously defined clusters. It’s a technique aimed more toward grouping data into a tree of clusters called dendrograms, which graphically represents the hierarchical relationship between the underlying clusters."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#comparison-of-hierarchical-clustering-to-other-clustering-techniques",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#comparison-of-hierarchical-clustering-to-other-clustering-techniques",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "Comparison of Hierarchical Clustering to Other Clustering Techniques",
    "text": "Comparison of Hierarchical Clustering to Other Clustering Techniques\nHierarchical clustering is a powerful algorithm, but it is not the only one out there, and each type of clustering comes with its set of advantages and drawbacks.\nLet’s understand how it compares to other types of clustering, such as K-means and model-based clustering. Many more techniques exist, but these two, in addition to hierarchical clustering, are widely used and provide a framework to help better understand the others.\nYou can learn more about clustering in machine learning in our separate article, covering five essential clustering algorithms.\n\nHierarchical clustering vs K Means clustering\nUnlike Hierarchical clustering, K-means clustering seeks to partition the original data points into “K” groups or clusters where the user specifies “K” in advance.\nThe general idea is to look for clusters that minimize the squared Euclidean distance of all the points from the centers over all attributes (variables or features) and merge those individuals in an iterative manner.\nOur K-means Clustering in Python with Scikit-learn tutorial will help you understand the inner workings of K-means clustering with an interesting case study.\n\nBenefits\n\nIt is computationally efficient compared to hierarchical clustering and can be used to analyze large data sets.\nK-means is easier to understand and implement.\n\n\n\nDrawbacks\n\nIt is less flexible than hierarchical clustering because it constrains the user to specify the number of clusters beforehand, which may not be obvious in some situations.\nThe result is not stable and changes from one iteration to another for the same data set.\nIt is more sensitive to outliers because the use of outliers in the data impacts the mean of the cluster.\nBoth k-means and hierarchical clustering are incapable of handling categorical data directly and may not work well with data that is not continuous or has very large variance.\n\nDespite its limitations, k-means clustering is still a popular method due to its ease of use and computational efficiency. It is frequently used as a reference point for comparing the performance of other clustering techniques.\n\n\n\nModel-based clustering\nBoth K-means and hierarchical clustering techniques use a distance matrix to represent the distances between all the points in the dataset. Model-based clustering, on the other hand, applies statistical techniques to identify clusters in the data. Below is the general process:\n\nDecide the statistical model to use and choose the number of clusters.\nFit the model on the data.\nIdentify the clusters based on the model parameters.\n\n\nBenefits\n\nModel-based clustering is more flexible than hierarchical clustering because it allows the use of different models to identify different types of clusters.\nIt works better on data with complex shapes or structures.\n\n\n\nDrawbacks\n\nIt’s computationally more expensive than hierarchical clustering, especially for large data.\nIt requires a better understanding of statistical modeling techniques since the choice of the model can affect the final result.\nSimilarly to K-means clustering, it requires the number of clusters to be specified beforehand."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#applications-of-hierarchical-clustering",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#applications-of-hierarchical-clustering",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "Applications of Hierarchical Clustering",
    "text": "Applications of Hierarchical Clustering\nHierarchical clustering has a variety of applications in our day-to-day life, including (but by no means limited to) biology, image processing, marketing, economics, and social network analysis.\n\nBiology\nThe clustering of DNA sequences is one of the biggest challenges in bioinformatics.\nBiologists can leverage hierarchical clustering to study genetic relationships between organisms to classify those organisms into taxonomic groups. This is beneficial for quick analysis and visualization of the underlying relationships.\n\n\nImage processing\nHierarchical clustering can be performed in image processing to group similar regions or pixels of an image in terms of color, intensity, or other features. This can be useful for further tasks such as image segmentation, image classification, and object recognition.\n\n\nMarketing\nMarketing specialists can use hierarchical clustering to draw a hierarchy between different types of customers based on their purchasing habits for better marketing strategies and product recommendations. For instance, different products in retails can be recommended to customers whether they are low, medium, or high spenders.\n\n\nSocial network analysis\nSocial networks are a great source of valuable information when exploited efficiently. Hierarchical clustering can be used to identify groups or communities and to understand their relationships to each other and the structure of the network as a whole."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#the-hierarchical-clustering-algorithm",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#the-hierarchical-clustering-algorithm",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "The Hierarchical Clustering Algorithm",
    "text": "The Hierarchical Clustering Algorithm\nIn this section, we will look at three main concepts. The steps of the hierarchical algorithm, a highlight of the two types of hierarchical clustering (agglomerative and divisive), and finally, some techniques to choose the right distance measure.\n\nSteps involved in the hierarchical clustering algorithm\nThe hierarchical clustering algorithm employs the use of distance measures to generate clusters. This generation process involves the following main steps:\n\n\n\nCreating Hierarchical Clustering Algorithm\n\n\nPreprocess the data by removing missing data and applying any additional tasks that make the data as clean as possible. This step is a more general one for most of the machine learning tasks.\n\nCompute the distance matrix containing the distance between each pair of data points using a particular distance metric such as Euclidean distance, Manhattan distance, or cosine similarity. But the default distance metric is the Euclidean one.\n\n2.Merge the two clusters that are the closest in distance.\n\nUpdate the distance matrix with regard to the new clusters.\nRepeat steps 1, 2, and 3 until all the clusters are merged together to create a single cluster.\n\n\n\nHierarchical clustering examples\nWe can consider agglomerative and divisive clustering as mirrors of each other. Let’s have a better look at how each one operates, along with a hierarchical clustering example and graphical visualization.\n\nAgglomerative hierarchical clustering\nThis first scenario corresponds to the approach explained above. It starts by considering each observation as a singleton cluster (cluster with only one data point). Then iteratively merges clusters until only one cluster is obtained. This process is also known as the bottom-up approach.\nAs showing in the illustration below:\n\nWe start by considering each animal to be its unique cluster.\nThen we generate three different clusters from those unique animals based on their similarities:\n\nBirds: Eagle and Peacock\nMammals: Lion and Bear\nMore than three leg animals: Spider and Scorpion.\n\nWe repeat the merging process to create the vertebrate cluster by combining the two most similar clusters: Birds and Mammals.\nAfter this step, the remaining two clusters, Vertebrate and More than three legs, are merged to create a single Animals cluster.\n\n\n\n\nDendrogram of Agglomerative Clustering Approach\n\n\nDendrogram of Agglomerative Clustering Approach\n\n\nDivisive clustering\nOn the other hand, divisive clustering is top-down because it starts by considering all the data points as a unique cluster. Then it separates them until all the data points are unique.\nFrom this divisive approach graphic:\n\nWe notice that the whole animal dataset is considered as a single bloc.\nThen, we divide that block into two clusters: Vertebrate and More than 3 legs.\nThe division process is iteratively applied to the previously created clusters until we get unique animals.\n\n\n\n\nDendrogram of Divisive Clustering Approach\n\n\nDendrogram of Divisive Clustering Approach\n\n\n\nChoosing the right distance measure\nYour choice of distance measure is a critical step in clustering, and it depends on the problem you’re trying to solve. Considering the following scenario, we could cluster students based on any number of approaches such as their:\n\nCountry of origin\nGender, either male or female\nPrevious academic background.\n\nThese are all valid clusters but differ in meaning.\nEven though Euclidean distance is the most common distance used in most clustering software, other distance measures such as Manhattan distance, Canberra distance, Pearson or Spearman correlation, and Minkowski distance exist.\n\n\nHow to measure clusters before merging them\nThe previously mentioned distances are related to items. In this section, we will cover three standard ways (not exhaustive) to measure the nearest pair of clusters before merging them: (1) Single linkage, (2) Complete linkage, and (3) Average linkage.\n\nSingle linkage\nFrom all the pairwise distances between the items in the two clusters C1 and C2, the single linkage takes the distance between the clusters as the minimum distance.\nDistance (C1, C2) = Min { d(i, j), where item i is within C1, and item j is within C2}\nOut of all the pairs of items from the two clusters, the ones highlighted in green have the minimum distance.\n\n\n\nSingle linkage illustration\n\n\nSingle linkage illustration\n\n\nComplete linkage\nFrom all the pairwise distances between the items in the two clusters C1 and C2, the single linkage takes the distance between the clusters as the maximum distance.\nDistance (C1, C2) = Max { d(i, j), where item i is within C1, and item j is within C2}\nOut of all the pairs of items from the two clusters, the ones highlighted in green have the maximum distance.\n\n\n\nComplete linkage illustration\n\n\nComplete linkage illustration\n\n\nAverage linkage\nIn the average linkage clustering, the distance between two given clusters C1 and C2 corresponds to the average distances between all pairs of items in the two clusters.\nDistance (C1, C2) = Sum{ d(i, j) } / Total Number of distances\n\n\n\nAverage linkage illustration\n\n\nAverage linkage illustration\nThen the average linkage clustering is performed as follows\nd(a,j) + d(a,h) + d(a,n) + d(d,j) + d(d,h) + d(d,n)\n————————————————————–, where Total number of distances = 6\nTotal number of distances"
  },
  {
    "objectID": "ders7.html#artificial-neural-networks",
    "href": "ders7.html#artificial-neural-networks",
    "title": "Artificial Neural Networks",
    "section": "Artificial Neural Networks",
    "text": "Artificial Neural Networks\nArtificial Neural Networks (ANN) are systems developed to automatically perform capabilities such as learning through new information, creating new information, and making discoveries - similar to characteristics of the human brain - without any external assistance.\nThe most basic task of an ANN is to determine an output set corresponding to an input set presented to it. To achieve this, the network is trained with examples of the relevant event (learning) to acquire generalization ability. Through this generalization, output sets corresponding to similar events are determined."
  },
  {
    "objectID": "ders7.html#biological-connection",
    "href": "ders7.html#biological-connection",
    "title": "Artificial Neural Networks",
    "section": "Biological Connection",
    "text": "Biological Connection\nA biological nerve cell is the fundamental building block that enables the human brain to function. A neural network is formed by billions of nerve cells coming together. An Artificial Neural Cell (ANC) mimics biological nerve cells to achieve learning and uncover relationships between phenomena. Information is transmitted between nerve cells in a manner similar to a real neural network."
  },
  {
    "objectID": "ders7.html#application-areas-of-anns",
    "href": "ders7.html#application-areas-of-anns",
    "title": "Artificial Neural Networks",
    "section": "Application Areas of ANNs",
    "text": "Application Areas of ANNs\n\nFunction Approximation\nTime Series Analysis\nRegression\nClassification"
  },
  {
    "objectID": "ders7.html#function-approximation",
    "href": "ders7.html#function-approximation",
    "title": "Artificial Neural Networks",
    "section": "Function Approximation",
    "text": "Function Approximation\nLet x = {x₁, x₂, …, xₙ} be a vector and y = f(x) be a function associated with it. Although the function is unknown, there are x vectors and corresponding y values. Thus, to find the y value according to a given xₙₑw, the function needs to be estimated (approximated). As seen, in an ANN, input neurons will be n in number representing x, while output neurons will be one in number representing y."
  },
  {
    "objectID": "ders7.html#time-series-analysis",
    "href": "ders7.html#time-series-analysis",
    "title": "Artificial Neural Networks",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nTime series future value prediction is a very general problem. There are various statistical approaches for such a problem. When time is represented by t, the function xᵗ = f(xᵗ⁻¹, xᵗ⁻², xᵗ⁻³, …) can be estimated to predict future values. Many business and economic time series analyses show seasonal and trend-based variations."
  },
  {
    "objectID": "ders7.html#regression",
    "href": "ders7.html#regression",
    "title": "Artificial Neural Networks",
    "section": "Regression",
    "text": "Regression\nRegression is the process of creating a prediction function between independent variables and the dependent variable. For ANN, independent variables are represented by input neurons, while dependent variables are represented by output neurons. While the number of independent variables is insignificant in regression analysis, the dependent variable can only be one. However, in ANNs, since both input and output neurons can be determined as desired, both the number of independent variables and dependent variables become insignificant."
  },
  {
    "objectID": "ders7.html#classification",
    "href": "ders7.html#classification",
    "title": "Artificial Neural Networks",
    "section": "Classification",
    "text": "Classification\nClassification can be defined as determining which group an object belongs to based on its characteristics. For example, in a bank, it’s possible to divide loan applicants into two classes such as “can be given credit” or “cannot be given credit” based on various characteristics like monthly income, assets, and education level. For this purpose, the ANN’s input neurons will represent the objects’ characteristics, while output neurons will represent the class it belongs to. In this example, there are two classes, and this situation is called binary decision. Assume that either logistic transformation function or hyperbolic tangent transformation function is used in the output neuron."
  },
  {
    "objectID": "ders7.html#classification-contd",
    "href": "ders7.html#classification-contd",
    "title": "Artificial Neural Networks",
    "section": "Classification (cont’d)",
    "text": "Classification (cont’d)\nIn cases where the number of classes increases, multiple cells may need to be used in the output layer. For example, in a situation with four classes, four cells can be used in the output layer, and each cell corresponds to one class. When using the logistic transformation function in each neuron, neurons with values close to 0 represent not belonging to the relevant class, while those close to 1 represent belonging to the relevant class."
  },
  {
    "objectID": "ders7.html#neural-cell-structure",
    "href": "ders7.html#neural-cell-structure",
    "title": "Artificial Neural Networks",
    "section": "Neural Cell Structure",
    "text": "Neural Cell Structure\nAn ANN consists of five parts\n\nInputs (x₁, x₂, …, xₙ)\nWeights (w₁, w₂, …, wₙ)\nCombination (summation) function\nActivation function\nOutput"
  },
  {
    "objectID": "ders7.html#ann-neuron-cell",
    "href": "ders7.html#ann-neuron-cell",
    "title": "Artificial Neural Networks",
    "section": "ANN neuron cell",
    "text": "ANN neuron cell\n\n\nimage source"
  },
  {
    "objectID": "ders7.html#common-combination-functions",
    "href": "ders7.html#common-combination-functions",
    "title": "Artificial Neural Networks",
    "section": "Common Combination Functions",
    "text": "Common Combination Functions\n\n\n\n\n\n\n\n\nFunction\nEquation\nDescription\n\n\n\n\nAddition\nNET = Σᵢ₌₁ⁿ xᵢwᵢ\nNET value is obtained by summing the products of input and weight values\n\n\nMultiplication\nNET = ∏ᵢ₌₁ⁿ xᵢwᵢ\nNET value is obtained by multiplying the products of input and weight values\n\n\nMaximum\nNET = Max(xᵢwᵢ)\nNET value is obtained from the largest of the products of input and weight values\n\n\nMinimum\nNET = Min(xᵢwᵢ)\nNET value is obtained from the smallest of the products of input and weight values\n\n\nSign\nNET = Sgn(xᵢwᵢ)\nThe number of positive and negative values from the products of input and weight values is found, and the larger value is accepted as the NET value"
  },
  {
    "objectID": "ders7.html#common-activation-functions",
    "href": "ders7.html#common-activation-functions",
    "title": "Artificial Neural Networks",
    "section": "Common Activation Functions",
    "text": "Common Activation Functions\nIdentity Function\n\nA commonly used linear function\ng(x) = x"
  },
  {
    "objectID": "ders7.html#common-activation-functions-1",
    "href": "ders7.html#common-activation-functions-1",
    "title": "Artificial Neural Networks",
    "section": "Common Activation Functions",
    "text": "Common Activation Functions\nStep Function\n\nGenerally used in single-layer networks\ng(x) = {\n\nb if x ≥ c\na if x &lt; c }\n\nWhere c = 0, a = 0, b = 1 or\nc = 0, a = -1, b = 1 are preferred"
  },
  {
    "objectID": "ders7.html#section",
    "href": "ders7.html#section",
    "title": "Artificial Neural Networks",
    "section": "",
    "text": "Step function for c=0, b=1 and a=-1\n\n\nimage source"
  },
  {
    "objectID": "ders7.html#common-activation-functions-2",
    "href": "ders7.html#common-activation-functions-2",
    "title": "Artificial Neural Networks",
    "section": "Common Activation Functions",
    "text": "Common Activation Functions\nSigmoid (Logistic) Function\n\nKeeps the value nᵢ in a cell within [0,1] range\nThe t parameter is used as a type of sensitivity level\nParticularly advantageous in networks trained with backpropagation\nSigmoid function output is in [0,1] range\ng(x) = 1/(1 + e⁻ˣ/ᵗ)"
  },
  {
    "objectID": "ders7.html#section-1",
    "href": "ders7.html#section-1",
    "title": "Artificial Neural Networks",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders7.html#common-activation-functions-3",
    "href": "ders7.html#common-activation-functions-3",
    "title": "Artificial Neural Networks",
    "section": "Common Activation Functions",
    "text": "Common Activation Functions\nHyperbolic Tangent Function\n\nSimilar to logistic activation function\nKeeps the value nᵢ in a cell within [-1,1] range\ng(x) = (eˣ/ᵗ - e⁻ˣ/ᵗ)/(eˣ/ᵗ + e⁻ˣ/ᵗ)"
  },
  {
    "objectID": "ders7.html#section-2",
    "href": "ders7.html#section-2",
    "title": "Artificial Neural Networks",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders7.html#summary-of-activation-functions",
    "href": "ders7.html#summary-of-activation-functions",
    "title": "Artificial Neural Networks",
    "section": "Summary of activation functions",
    "text": "Summary of activation functions\nsee the next slide\n\nimage source"
  },
  {
    "objectID": "ders7.html#perceptron",
    "href": "ders7.html#perceptron",
    "title": "Artificial Neural Networks",
    "section": "Perceptron",
    "text": "Perceptron\n\n\nimage source"
  },
  {
    "objectID": "ders7.html#ann",
    "href": "ders7.html#ann",
    "title": "Artificial Neural Networks",
    "section": "ANN",
    "text": "ANN\n\n\nimage source"
  },
  {
    "objectID": "ders7.html#interactive-demos",
    "href": "ders7.html#interactive-demos",
    "title": "Artificial Neural Networks",
    "section": "Interactive demos",
    "text": "Interactive demos\nML Playground\nNeural Network Playground"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Makine Öğrenmesinde Sınıflandırma Yöntemleri ve R Uygulamaları, Ersoy Öz & Selçuk Alp, Nobel Akademik Yayıncılık, 2020.\nVeri Madenciliği ve Makine Öğrenmesi İle Farklı Alanlarda Uygulamalar, Şenol Çelik, Nilay Köleoğlu, & Fatih Çemrek, Holistence Publications, 2022\nSağlık Bilimlerinde Yapay Zeka, Ahmet Rıza Şahin, Kamil Doğan, & Süleyman Sivri, Akademisyen Kitabevi A.Ş., 2020\nYapay Zeka, Atınç Yılmaz, Kodlab Yayın Dağıtım Yazılım Ltd. Şti., 2022\nMachine Learning Algorithms, Giuseppe Bonaccorso, Packt Publishing, 2017.\nOptimizasyon Modelleme ve Yapay Zeka Algoritmaları, Bayram Köse, Bahar Demirtürk, Efe Akademi Yayınları, 2023\n\n\n\n\n3 Ekim 2024 16:00\nİçerik: İlk derste, ChatGPT ve benzeri büyük dil modelleri tanıtıldı. OpenAI Playground üzerinde “cümle tamamlama” kavramı ve “temparature” ayarı gösterildi. Daha iyi çıktı almak için Prompt ayarları anlatıldı. Dökümandan sorgu/chat yapmak için Gemini ve Claude örnekleri verildi. Yine Claude üzerinden kod yazdırma örneği işlendi. İngilizce Sunum linki\n\n\n\n10 Ekim 2024 16:00\nİçerik: ChatGPT ve ötesi. İngilizce Sunum linki\n\n\n\n17 Ekim 2024 16:00\nİçerik: Veri türleri ve verilerin işlenmesine dair bilgi verildi. Performans ölçütleri konusuna başlandı. Sunum linki1 ve Sunum link2\n\n\n\n24 Ekim 2024 16:00\nİçerik: Performans ölçütleri konusuna devam edildi. Optimizasyon konusunda başlandı. Performans sunum link Optimizasyon sunum link\n\n\n\n31 Ekim 2024 16:00\nİçerik: Optimizasyon konusuna devam edildi. Optimizasyon sunum link\n\n\n\n7 Kasım 2024 16:00\nİçerik: Genetik algoritmalar sunum link\n\n\n\n14 Kasım Perşembe 16:00\nİçerik: Yapay sinir ağları Sunum linki.\n\n\n\n28 Kasım Perşembe 16:00\nİçerik: Sınıflandırma Sunum linki.\n\n\n\n5 Aralık Perşembe 16:00\nİçerik: Sınıflandırma devam Sunum linki.\n\n\n\n12 Aralılk Perşembe 16:00\nİçerik: Regresyon Sunum linki.\n\n\n\n19 Aralılk Perşembe 16:00\nİçerik: Kümeleme Sunum linki.\n\n\n\n2 Ocak Perşembe 16:00\nİçerik: Doğal Dil İşleme, NLP Sunum linki."
  },
  {
    "objectID": "index.html#kaynaklar",
    "href": "index.html#kaynaklar",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Makine Öğrenmesinde Sınıflandırma Yöntemleri ve R Uygulamaları, Ersoy Öz & Selçuk Alp, Nobel Akademik Yayıncılık, 2020.\nVeri Madenciliği ve Makine Öğrenmesi İle Farklı Alanlarda Uygulamalar, Şenol Çelik, Nilay Köleoğlu, & Fatih Çemrek, Holistence Publications, 2022\nSağlık Bilimlerinde Yapay Zeka, Ahmet Rıza Şahin, Kamil Doğan, & Süleyman Sivri, Akademisyen Kitabevi A.Ş., 2020\nYapay Zeka, Atınç Yılmaz, Kodlab Yayın Dağıtım Yazılım Ltd. Şti., 2022\nMachine Learning Algorithms, Giuseppe Bonaccorso, Packt Publishing, 2017.\nOptimizasyon Modelleme ve Yapay Zeka Algoritmaları, Bayram Köse, Bahar Demirtürk, Efe Akademi Yayınları, 2023"
  },
  {
    "objectID": "index.html#ders1",
    "href": "index.html#ders1",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "3 Ekim 2024 16:00\nİçerik: İlk derste, ChatGPT ve benzeri büyük dil modelleri tanıtıldı. OpenAI Playground üzerinde “cümle tamamlama” kavramı ve “temparature” ayarı gösterildi. Daha iyi çıktı almak için Prompt ayarları anlatıldı. Dökümandan sorgu/chat yapmak için Gemini ve Claude örnekleri verildi. Yine Claude üzerinden kod yazdırma örneği işlendi. İngilizce Sunum linki"
  },
  {
    "objectID": "index.html#ders2",
    "href": "index.html#ders2",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "10 Ekim 2024 16:00\nİçerik: ChatGPT ve ötesi. İngilizce Sunum linki"
  },
  {
    "objectID": "index.html#ders3",
    "href": "index.html#ders3",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "17 Ekim 2024 16:00\nİçerik: Veri türleri ve verilerin işlenmesine dair bilgi verildi. Performans ölçütleri konusuna başlandı. Sunum linki1 ve Sunum link2"
  },
  {
    "objectID": "index.html#ders4",
    "href": "index.html#ders4",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "24 Ekim 2024 16:00\nİçerik: Performans ölçütleri konusuna devam edildi. Optimizasyon konusunda başlandı. Performans sunum link Optimizasyon sunum link"
  },
  {
    "objectID": "index.html#ders5",
    "href": "index.html#ders5",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "31 Ekim 2024 16:00\nİçerik: Optimizasyon konusuna devam edildi. Optimizasyon sunum link"
  },
  {
    "objectID": "index.html#ders6",
    "href": "index.html#ders6",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "7 Kasım 2024 16:00\nİçerik: Genetik algoritmalar sunum link"
  },
  {
    "objectID": "index.html#ders7",
    "href": "index.html#ders7",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "14 Kasım Perşembe 16:00\nİçerik: Yapay sinir ağları Sunum linki."
  },
  {
    "objectID": "index.html#ders8",
    "href": "index.html#ders8",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "28 Kasım Perşembe 16:00\nİçerik: Sınıflandırma Sunum linki."
  },
  {
    "objectID": "index.html#ders9",
    "href": "index.html#ders9",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "5 Aralık Perşembe 16:00\nİçerik: Sınıflandırma devam Sunum linki."
  },
  {
    "objectID": "index.html#ders10",
    "href": "index.html#ders10",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "12 Aralılk Perşembe 16:00\nİçerik: Regresyon Sunum linki."
  },
  {
    "objectID": "index.html#ders11",
    "href": "index.html#ders11",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "19 Aralılk Perşembe 16:00\nİçerik: Kümeleme Sunum linki."
  },
  {
    "objectID": "index.html#ders12",
    "href": "index.html#ders12",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "2 Ocak Perşembe 16:00\nİçerik: Doğal Dil İşleme, NLP Sunum linki."
  },
  {
    "objectID": "Hierarchical_Clustering_in_Data_Mining.html",
    "href": "Hierarchical_Clustering_in_Data_Mining.html",
    "title": "Hierarchical Clustering in Data Mining",
    "section": "",
    "text": "Contents are taken from the Article “Hierarchical Clustering in Data Mining”\nA *Hierarchical clustering* method works via grouping data into a tree of clusters. Hierarchical clustering begins by treating every data point as a separate cluster. Then, it repeatedly executes the subsequent steps:\nIn Hierarchical Clustering, the aim is to produce a hierarchical series of nested clusters. A diagram called *Dendrogram* (A Dendrogram is a tree-like diagram that statistics the sequences of merges or splits) graphically represents this hierarchy and is an inverted tree that describes the order in which factors are merged (bottom-up view) or clusters are broken up (top-down view)."
  },
  {
    "objectID": "Hierarchical_Clustering_in_Data_Mining.html#what-is-hierarchical-clustering",
    "href": "Hierarchical_Clustering_in_Data_Mining.html#what-is-hierarchical-clustering",
    "title": "Hierarchical Clustering in Data Mining",
    "section": "What is Hierarchical Clustering?",
    "text": "What is Hierarchical Clustering?\nHierarchical clustering is a method of cluster analysis in data mining that creates a hierarchical representation of the clusters in a dataset. The method starts by treating each data point as a separate cluster and then iteratively combines the closest clusters until a stopping criterion is reached. The result of hierarchical clustering is a tree-like structure, called a dendrogram, which illustrates the hierarchical relationships among the clusters.\n\n*Hierarchical clustering has several advantages over other clustering methods*\n\nThe ability to handle non-convex clusters and clusters of different sizes and densities.\nThe ability to handle missing data and noisy data.\nThe ability to reveal the hierarchical structure of the data, which can be useful for understanding the relationships among the clusters.\n\n\n\n*Drawbacks of Hierarchical Clustering*\n\nThe need for a criterion to stop the clustering process and determine the final number of clusters.\nThe computational cost and memory requirements of the method can be high, especially for large datasets.\nThe results can be sensitive to the initial conditions, linkage criterion, and distance metric used. In summary, Hierarchical clustering is a method of data mining that groups similar data points into clusters by creating a hierarchical structure of the clusters.\nThis method can handle different types of data and reveal the relationships among the clusters. However, it can have high computational cost and results can be sensitive to some conditions.\n\n\n\nTypes of Hierarchical Clustering\nBasically, there are two types of hierarchical Clustering:\n\nAgglomerative Clustering\nDivisive clustering\n\n\n\n1. Agglomerative Clustering\nInitially consider every data point as an *individual* Cluster and at every step, merge the nearest pairs of the cluster. (It is a bottom-up method). At first, every dataset is considered an individual entity or cluster. At every iteration, the clusters merge with different clusters until one cluster is formed.\n\nThe algorithm for Agglomerative Hierarchical Clustering is:\n\nCalculate the similarity of one cluster with all the other clusters (calculate proximity matrix)\nConsider every data point as an individual cluster\nMerge the clusters which are highly similar or close to each other.\nRecalculate the proximity matrix for each cluster\nRepeat Steps 3 and 4 until only a single cluster remains.\n\nLet’s see the graphical representation of this algorithm using a dendrogram.\n\n*Note:* This is just a demonstration of how the actual algorithm works no calculation has been performed below all the proximity among the clusters is assumed.\n\nLet’s say we have six data points *A, B, C, D, E, and F*.\n\n\n\nAgglomerative Hierarchical clustering\n\n\nAgglomerative Hierarchical clustering\n\n*Step-1:* Consider each alphabet as a single cluster and calculate the distance of one cluster from all the other clusters.\n*Step-2:* In the second step comparable clusters are merged together to form a single cluster. Let’s say cluster (B) and cluster (C) are very similar to each other therefore we merge them in the second step similarly to cluster (D) and (E) and at last, we get the clusters [(A), (BC), (DE), (F)]\n*Step-3:* We recalculate the proximity according to the algorithm and merge the two nearest clusters([(DE), (F)]) together to form new clusters as [(A), (BC), (DEF)]\n*Step-4:* Repeating the same process; The clusters DEF and BC are comparable and merged together to form a new cluster. We’re now left with clusters [(A), (BCDEF)].\n*Step-5:* At last, the two remaining clusters are merged together to form a single cluster [(ABCDEF)].\n\n\n\n\n2. Divisive Hierarchical clustering\nWe can say that Divisive Hierarchical clustering is precisely the *opposite* of Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we take into account all of the data points as a single cluster and in every iteration, we separate the data points from the clusters which aren’t comparable. In the end, we are left with N clusters.\n\n\n\nDivisive Hierarchical clustering\n\n\nDivisive Hierarchical clustering"
  },
  {
    "objectID": "ders3.html#değerlendirme-ölçütleri-performance-metrics",
    "href": "ders3.html#değerlendirme-ölçütleri-performance-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Değerlendirme ölçütleri / Performance Metrics",
    "text": "Değerlendirme ölçütleri / Performance Metrics\n\nSınıflandırma değerlendirme (Classification)\nRegresyon değerlendirme (Regression)"
  },
  {
    "objectID": "ders3.html#sınıflandırma-ve-regresyon-classification-vs-regression",
    "href": "ders3.html#sınıflandırma-ve-regresyon-classification-vs-regression",
    "title": "Değerlendirme Ölçütleri",
    "section": "Sınıflandırma ve Regresyon (Classification vs regression)",
    "text": "Sınıflandırma ve Regresyon (Classification vs regression)\n\nSource"
  },
  {
    "objectID": "ders3.html#section",
    "href": "ders3.html#section",
    "title": "Değerlendirme Ölçütleri",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders3.html#sınıflandırma-ve-regresyon",
    "href": "ders3.html#sınıflandırma-ve-regresyon",
    "title": "Değerlendirme Ölçütleri",
    "section": "Sınıflandırma ve Regresyon",
    "text": "Sınıflandırma ve Regresyon\n\nSource"
  },
  {
    "objectID": "ders3.html#section-1",
    "href": "ders3.html#section-1",
    "title": "Değerlendirme Ölçütleri",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders3.html#fit",
    "href": "ders3.html#fit",
    "title": "Değerlendirme Ölçütleri",
    "section": "Fit",
    "text": "Fit\n\nSource"
  },
  {
    "objectID": "ders3.html#training",
    "href": "ders3.html#training",
    "title": "Değerlendirme Ölçütleri",
    "section": "Training",
    "text": "Training\n\nSource"
  },
  {
    "objectID": "ders3.html#training-test",
    "href": "ders3.html#training-test",
    "title": "Değerlendirme Ölçütleri",
    "section": "Training / Test",
    "text": "Training / Test\n\nSource"
  },
  {
    "objectID": "ders3.html#classification-performance-metrics",
    "href": "ders3.html#classification-performance-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Classification Performance Metrics",
    "text": "Classification Performance Metrics\nBir Sınıflandırma modeli oluşturulduktan sonra bu model ile yapılan tahminlerin ne kadar doğru olduğuna dair değerlendirme yapılması gereklidir.\nAşağıda verilen confusion matrix (karşılaştırma matrisi) bir sınıflandırma modeline dair gerçekleşen durumları ve bu durumlara dair tahminleri verilmiştir.\n\n\n\n\n\nActual (Gerçek)\n\n\n\n\n\n\n\nTrue (Doğru)\nFalse (Yanlış)\n\n\nPrediction (Tahmin)\nPozitif\nTP\nFP\n\n\n\nNegatif\nFN\nTN\n\n\n\nTP : True Positive, FP : False Positive, FN : False Negative, TN : True Negative\nTrue ve false değeri bu modele dair gerçek sonuçları, positive ve negative ise modele dair tahminleri göstermektedir."
  },
  {
    "objectID": "ders3.html#classification-performance-metrics-1",
    "href": "ders3.html#classification-performance-metrics-1",
    "title": "Değerlendirme Ölçütleri",
    "section": "Classification Performance Metrics",
    "text": "Classification Performance Metrics\nTümör ve Hasta örnekleri ele alırsak, Normal örnekler negatif olarak, Tümör örnekleri de pozitif olarak değerlendirilebilir.\n\nTP : Gerçekte Tümör olan hastayı (true/pozitif) Tümör olarak tahmin etmek (pozitif).\n\nFP : Gerçekte Normal olan örneği (false/negatif) Tümör olarak tahmin etmek (pozitif). — &gt; Type 1 Error\n\nFN : Gerçekte Tümor olan örneği (true/pozitif) Normal olarak tahmin etmek (negatif). — &gt; Type 2 Error\n\nTN : Gerçekte Normal olan örneği (false/negatif) Normal olarak tahmin etmek (negatif)."
  },
  {
    "objectID": "ders3.html#metrics",
    "href": "ders3.html#metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Metrics",
    "text": "Metrics\nAccuracy (Doğruluk) : Doğru tahminlerin toplam veri kümesine oranıdır.\nPrecision (Kesinlik): Pozitif olarak tahmin edilen verilerin kaçının gerçekten pozitif olduğunu gösterir.\nRecall or Sensitivity (Duyarlılık): Geliştirilen modelin pozitif olanların kaçını yakaladığını gösterir.\nF1 Score (F1 Skoru): F1 score, precision ve recall değerlerinin harmonik ortalamasıdır. Sınıf dağılımı benzer olduğunda accuracy kullanılabilirken, dengesiz veri setleri söz konusu olduğunda F1 skor daha iyi bir metriktir.\nROC Curve (ROC Eğrisi): Yanlış pozitif oranı ve gerçek pozitif oranı göz önünde bulundurarak x ekseninde ve y ekseninde 0’dan 100’e kadar olan değerlerin üzerinde bir eğri oluşturulur. Bu eğrinin altında kalan alana Area Under Curve (AUC) adı verilir. Bu alanın büyük olması modelin başarılı olduğunu gösterir. Grafikte yer alan mavi çizgi; ne kadar geniş bir alan kaplıyorsa modelin tahmin başarısı o kadar yüksek, ortadaki kesikli çizgiye ne kadar yakınsa modelin başarı oranı o kadar düşüktür."
  },
  {
    "objectID": "ders3.html#roc-curve",
    "href": "ders3.html#roc-curve",
    "title": "Değerlendirme Ölçütleri",
    "section": "ROC Curve",
    "text": "ROC Curve"
  },
  {
    "objectID": "ders3.html#why-too-many-metrics",
    "href": "ders3.html#why-too-many-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Why too many metrics?",
    "text": "Why too many metrics?\n\n\nNeden birden fazla metrik kullanılıyor, bir örnek ile görelim. Yandaki durum için, 8 Normal ve 2 Tümör olan bir durumda, her örnek için Normal diye tahminde bulunursak. True Negatif (TN) sayısı 8 ve False Negatif (FN) sayısı ise 2 olmaktadır.\nBu durumda, doğruluk, accuracy\nACC = (TP + TN) / (TP + FP + TN + FN)\nformülünden dolayı 8 / 10 = 0.8 yani %80 olarak hesaplanmaktadır.\nFakat, Duyarlılık/Sensitivity\nSENS = TP / (TP + FN)\nformülünden 0 çıkmaktadır.\n\n\n\n\nActual Label\nPrediction\n\n\n\n\nTumor\nNormal\n\n\nTumor\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal"
  },
  {
    "objectID": "ders3.html#example",
    "href": "ders3.html#example",
    "title": "Değerlendirme Ölçütleri",
    "section": "Example",
    "text": "Example\n100 kişilik bir çalışmada, 25 Tümör hastası ve 75 Normal birey bulunmaktadır. Normal (negatif) olan 70 kişi Normal olarak tahmin edilmştir (TN: True Negatif). 5 kişi Normal olduğu halde Tümör olarak tahmin edilmiştir (FP: False pozitif). Gerçekte Tümör olan 15 kişi Tümör olarak tahmin edilmiştir (TP: True pozitif). Son olarak, 10 kişi Tümör olduğu halde Normal olarak tahmin edilmiştir (FN: False negatif). Bu duruma göre Confusion Matrix aşağıdaki gibi hesaplanacaktır.\n\n\n\n\n\nActual\n\n\n\n\n\n\n\nTrue\nFalse\n\n\nTahmin\nPozitif\nTP (15)\nFP (5)\n\n\n\nNegatif\nFN (10)\nTN (70)"
  },
  {
    "objectID": "ders3.html#example-1",
    "href": "ders3.html#example-1",
    "title": "Değerlendirme Ölçütleri",
    "section": "Example",
    "text": "Example\nBu durumda,\nDoğruluk (Accuracy) ACC = (TP + TN) / (TP + TN + FN + FP) = 0.85\nDuyarlılık (Sensitivity) SENS = TP / (TP + FN) = 0.60\nKesinlik (Precision) PREC = TP / (TP + FP) = 0.75\nF1 Skor = 2 x (PREC x SENS) / (PREC + SENS) = 0.6667"
  },
  {
    "objectID": "ders3.html#summary",
    "href": "ders3.html#summary",
    "title": "Değerlendirme Ölçütleri",
    "section": "Summary",
    "text": "Summary\n\nSource"
  },
  {
    "objectID": "ders3.html#python-code",
    "href": "ders3.html#python-code",
    "title": "Değerlendirme Ölçütleri",
    "section": "Python code",
    "text": "Python code\n\nfrom sklearn.metrics import confusion_matrix\n\nactual    = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]\npredicted = [0, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n\nprint(\"\\nConfusion matrix\")\nconf_mat = confusion_matrix(actual, predicted) \n\nprint(conf_mat)\n\n\nConfusion matrix\n[[1 3]\n [2 4]]"
  },
  {
    "objectID": "ders3.html#regression-performance-metrics",
    "href": "ders3.html#regression-performance-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Regression Performance Metrics",
    "text": "Regression Performance Metrics\nFollowing metrics can be used to measure the performance of regression model output.\n\nMean Error (ME)\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nThere are much more advanced metrics but we’ll learn only essental ones"
  },
  {
    "objectID": "ders3.html#mean-error-me",
    "href": "ders3.html#mean-error-me",
    "title": "Değerlendirme Ölçütleri",
    "section": "Mean Error (ME)",
    "text": "Mean Error (ME)\n\\[ \\text{ME} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\]\n\\(y_i\\): Actual value, \\(\\hat{y}_i\\): predicted value, n: number of observations"
  },
  {
    "objectID": "ders3.html#mean-absolute-error-mae",
    "href": "ders3.html#mean-absolute-error-mae",
    "title": "Değerlendirme Ölçütleri",
    "section": "Mean Absolute Error (MAE)",
    "text": "Mean Absolute Error (MAE)\nAdding negative results is not right when using ME. Let’s take care of it.\n\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]"
  },
  {
    "objectID": "ders3.html#mean-squared-error-mse",
    "href": "ders3.html#mean-squared-error-mse",
    "title": "Değerlendirme Ölçütleri",
    "section": "Mean Squared Error (MSE)",
    "text": "Mean Squared Error (MSE)\n\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nPlease note that squaring the error with punish the model more. Also, squaring a difference will take care of negative sign."
  },
  {
    "objectID": "ders3.html#root-mean-squared-error-rmse",
    "href": "ders3.html#root-mean-squared-error-rmse",
    "title": "Değerlendirme Ölçütleri",
    "section": "Root Mean Squared Error (RMSE)",
    "text": "Root Mean Squared Error (RMSE)\n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]"
  },
  {
    "objectID": "kmeans-r.html",
    "href": "kmeans-r.html",
    "title": "K-means clustering with R",
    "section": "",
    "text": "taken from Partitional Clustering in R: The Essentials"
  },
  {
    "objectID": "kmeans-r.html#the-data",
    "href": "kmeans-r.html#the-data",
    "title": "K-means clustering with R",
    "section": "The data",
    "text": "The data\n\nlibrary(factoextra)\ndata(\"USArrests\")\ndf &lt;- scale(USArrests)\n\n\nhead(df)\n\n               Murder   Assault   UrbanPop         Rape\nAlabama    1.24256408 0.7828393 -0.5209066 -0.003416473\nAlaska     0.50786248 1.1068225 -1.2117642  2.484202941\nArizona    0.07163341 1.4788032  0.9989801  1.042878388\nArkansas   0.23234938 0.2308680 -1.0735927 -0.184916602\nCalifornia 0.27826823 1.2628144  1.7589234  2.067820292\nColorado   0.02571456 0.3988593  0.8608085  1.864967207"
  },
  {
    "objectID": "kmeans-r.html#what-should-k-be",
    "href": "kmeans-r.html#what-should-k-be",
    "title": "K-means clustering with R",
    "section": "What should K be?",
    "text": "What should K be?\n\nfviz_nbclust(df, kmeans, \"wss\")"
  },
  {
    "objectID": "kmeans-r.html#clustering",
    "href": "kmeans-r.html#clustering",
    "title": "K-means clustering with R",
    "section": "Clustering",
    "text": "Clustering\n\nkm.res &lt;- kmeans(df, 4, nstart=25)\nprint(km.res)\n\nK-means clustering with 4 clusters of sizes 16, 8, 13, 13\n\nCluster means:\n      Murder    Assault   UrbanPop        Rape\n1 -0.4894375 -0.3826001  0.5758298 -0.26165379\n2  1.4118898  0.8743346 -0.8145211  0.01927104\n3 -0.9615407 -1.1066010 -0.9301069 -0.96676331\n4  0.6950701  1.0394414  0.7226370  1.27693964\n\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             2              4              4              2              4 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             4              1              1              4              2 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             1              3              4              1              3 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             1              3              2              3              4 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             1              4              3              2              4 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             3              3              4              3              1 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             4              4              2              3              1 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             1              1              1              1              2 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             3              2              4              1              3 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             1              1              3              3              1 \n\nWithin cluster sum of squares by cluster:\n[1] 16.212213  8.316061 11.952463 19.922437\n (between_SS / total_SS =  71.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "kmeans-r.html#visualizing-the-results",
    "href": "kmeans-r.html#visualizing-the-results",
    "title": "K-means clustering with R",
    "section": "Visualizing the results",
    "text": "Visualizing the results\n\nfviz_cluster(km.res, data=df)"
  },
  {
    "objectID": "ders12-nlp.html#contents",
    "href": "ders12-nlp.html#contents",
    "title": "NLP - Natural Language Processing",
    "section": "Contents",
    "text": "Contents\n\n\n\nDefinition and why we need NLP\nCommon NLP Applications\n\nText summarization\nSentiment analysis\nKeyword extraction and association\nDocument classification\nNamed entity recognition\nMachine translation\nQuestion answering systems\nChatbots and conversational AI\n\n\n\n\nNLP Approaches\n\nTerm-based approaches\nDictionary-based approaches\nCorpus-based approaches\nStatistical approaches\nSemantic approaches\nDeep learning approaches"
  },
  {
    "objectID": "ders12-nlp.html#contents-contd",
    "href": "ders12-nlp.html#contents-contd",
    "title": "NLP - Natural Language Processing",
    "section": "Contents (cont’d)",
    "text": "Contents (cont’d)\n\nPreprocessing Steps in NLP: Text cleaning and normalization, Tokenization, Stop word removal, Stemming and lemmatization, Part-of-speech (POS) tagging, Syntax analysis and parsing, Named entity recognition\nFeature Extraction and Representation: Bag of Words (BoW) model, Term Frequency-Inverse Document Frequency (TF-IDF), N-gram models, Word embeddings (Word2Vec, FastText), Contextual embeddings (e.g., BERT, GPT)"
  },
  {
    "objectID": "ders12-nlp.html#definition",
    "href": "ders12-nlp.html#definition",
    "title": "NLP - Natural Language Processing",
    "section": "Definition",
    "text": "Definition\nNatural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and human language. It combines computational linguistics, machine learning, and deep learning techniques to enable computers to understand, interpret, generate, and respond to human language in a valuable way. NLP aims to bridge the gap between human communication and computer understanding by processing and analyzing large amounts of natural language data. This field encompasses various tasks such as speech recognition, text analysis, language translation, sentiment analysis, and text generation, allowing machines to derive meaning from human languages and respond in a way that is both meaningful and contextually appropriate."
  },
  {
    "objectID": "ders12-nlp.html#why-nlp",
    "href": "ders12-nlp.html#why-nlp",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?\nNatural Language Processing (NLP) is crucial because text data is fundamentally different from the structured numerical and categorical data typically used in traditional machine learning tasks. Here’s why NLP is necessary:\n\nUnstructured nature of text: Unlike tabular data where each column represents a specific feature, text is inherently unstructured. A sentence or document doesn’t come with predefined features or categories. This unstructured nature makes it challenging for computers to directly process and analyze text data.\nHigh dimensionality: Text data can be extremely high-dimensional. In a bag-of-words model, for instance, each unique word in the corpus becomes a feature. This can lead to thousands or even millions of dimensions, far exceeding what’s typically seen in structured data problems.\nSemantic complexity: Words and phrases can have multiple meanings depending on context. For example, “bank” could refer to a financial institution or the side of a river. Traditional ML models aren’t equipped to handle this semantic complexity without NLP techniques."
  },
  {
    "objectID": "ders12-nlp.html#why-nlp-1",
    "href": "ders12-nlp.html#why-nlp-1",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?\n\nSyntactic variability: The same meaning can be expressed in many different ways in natural language. For example, “The cat sat on the mat” and “On the mat sat the cat” convey the same information but have different word orders. ML models need NLP techniques to understand these variations.\nDealing with noise: Text data often contains “noise” such as spelling errors, informal language, or irrelevant information. NLP provides methods to clean and preprocess this data, making it suitable for analysis.\nFeature extraction: Unlike numerical data where features are explicit, textual features need to be extracted. NLP provides techniques like TF-IDF, word embeddings, and topic modeling to convert text into meaningful numerical representations that ML models can work with.\nHandling sequence information: Many ML models assume independence between features, but in text, the order of words matters. NLP techniques like recurrent neural networks or transformers are designed to capture this sequential information."
  },
  {
    "objectID": "ders12-nlp.html#why-nlp-2",
    "href": "ders12-nlp.html#why-nlp-2",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?\n\nMultilingual challenges: Text data can come in multiple languages, each with its own rules and nuances. NLP provides tools for language detection, translation, and cross-lingual analysis.\nDealing with context: Understanding text often requires grasping context beyond the immediate words. NLP techniques help in capturing and utilizing this broader context.\n\nIn essence, NLP is needed because it provides the necessary tools and techniques to transform the rich, complex, and unstructured nature of human language into a form that computers can process effectively. It bridges the gap between the way humans communicate and the way machines process information, enabling a wide range of applications from sentiment analysis to machine translation, chatbots, and beyond."
  },
  {
    "objectID": "ders12-nlp.html#why-nlp-3",
    "href": "ders12-nlp.html#why-nlp-3",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?"
  },
  {
    "objectID": "ders12-nlp.html#section",
    "href": "ders12-nlp.html#section",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders12-nlp.html#section-1",
    "href": "ders12-nlp.html#section-1",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Also, recordings is a text input after Speech2Text processing\nimage source"
  },
  {
    "objectID": "ders12-nlp.html#section-2",
    "href": "ders12-nlp.html#section-2",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Common NLP Applications - Summarization\n\nText summarization is an NLP application that condenses large volumes of text into brief, coherent summaries. For humans, summarizing involves understanding context, identifying key points, and rephrasing ideas - tasks that come naturally to us. However, for computers, this process is remarkably complex. A computer must analyze the text’s structure, grasp semantic meanings, determine the relative importance of different sentences or concepts, and then generate or extract a coherent summary. This requires sophisticated algorithms to mimic human-like understanding of language, context, and relevance. The challenge lies in teaching machines to distinguish between crucial information and extraneous details, a task that involves not just processing words, but understanding their relationships and significance within the broader context of the document.\n\nimage source"
  },
  {
    "objectID": "ders12-nlp.html#section-3",
    "href": "ders12-nlp.html#section-3",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Common NLP Applications - Sentiment Analysis\nSentiment analysis is an NLP technique that aims to determine the emotional tone behind a piece of text. While humans can often intuitively sense the sentiment in a statement, for computers, this task is surprisingly complex. A computer must go beyond simple keyword matching to grasp the overall emotional context. For instance, the phrase “This movie is sick!” could be positive or negative depending on the context and speaker.\nPlease visit https://hedonometer.org which tracks happiness in US using Twitter/X"
  },
  {
    "objectID": "ders12-nlp.html#section-4",
    "href": "ders12-nlp.html#section-4",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Common NLP Applications - Named Entity Recognition\n\nis an NLP technique that seeks to locate and classify named entities in text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. While humans can easily identify and categorize these entities in text, for computers, this task presents great challenges.\nPlease visit Geeks4Geeks Named Entity Recognition for demonstration."
  },
  {
    "objectID": "ders12-nlp.html#nlp-approaches---term-based",
    "href": "ders12-nlp.html#nlp-approaches---term-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Term based",
    "text": "NLP Approaches - Term based\nTerm-based approaches in NLP represent one of the fundamental methods for processing and analyzing text data. These approaches treat individual words or terms as the basic units of analysis. The core idea is to represent documents as collections of terms, often disregarding grammar and word order but keeping multiplicity. A common implementation is the “bag-of-words” model, where each document is represented as a vector of term frequencies. While simple, this method can be surprisingly effective for many tasks like document classification or information retrieval. However, term-based approaches face challenges with synonymy (different words with the same meaning) and polysemy (same word with multiple meanings). They also struggle to capture context and relationships between words. Despite these limitations, term-based methods serve as a foundation for more advanced NLP techniques and remain relevant in many applications due to their computational efficiency and interpretability"
  },
  {
    "objectID": "ders12-nlp.html#section-5",
    "href": "ders12-nlp.html#section-5",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders12-nlp.html#nlp-approaches---dictionary-based",
    "href": "ders12-nlp.html#nlp-approaches---dictionary-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Dictionary based",
    "text": "NLP Approaches - Dictionary based\nDictionary-based approaches in NLP rely on pre-existing lexical resources, such as dictionaries, thesauri, or ontologies, to process and analyze text. These methods leverage human-curated knowledge bases to understand and interpret language. In essence, they attempt to mimic how humans use dictionaries to comprehend unfamiliar words or concepts. For instance, in sentiment analysis, a dictionary-based approach might use a lexicon of words labeled with their associated sentiments."
  },
  {
    "objectID": "ders12-nlp.html#section-6",
    "href": "ders12-nlp.html#section-6",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders12-nlp.html#nlp-approaches---corpus-based",
    "href": "ders12-nlp.html#nlp-approaches---corpus-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Corpus based",
    "text": "NLP Approaches - Corpus based\nCorpus-based approaches in NLP rely on large collections of text data, known as corpora, to derive linguistic knowledge and patterns. Unlike dictionary-based methods that depend on pre-existing knowledge bases, corpus-based approaches learn from real-world language usage. These methods use statistical techniques to analyze how words and phrases are used in context across vast amounts of text. For example, word embeddings like Word2Vec or GloVe use neural networks to learn vector representations of words based on their co-occurrence patterns in large corpora.\nCorpus-based methods excel at capturing nuances in language use, handling ambiguity, and adapting to different domains or languages. They’re particularly powerful for tasks like language modeling, machine translation, and text generation."
  },
  {
    "objectID": "ders12-nlp.html#nlp-approaches---semantic-based",
    "href": "ders12-nlp.html#nlp-approaches---semantic-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Semantic based",
    "text": "NLP Approaches - Semantic based\nSemantic approaches in NLP focus on understanding the meaning of language, rather than just its structure or statistical patterns. These methods aim to capture and represent the underlying concepts and relationships in text, mirroring human comprehension. Semantic approaches often utilize knowledge graphs, ontologies, or semantic networks to model relationships between concepts. For example, in a biomedical context, a semantic approach might understand that “aspirin” is a type of “pain reliever” which “treats” “headaches”. These methods can handle complex queries, infer new knowledge, and understand context-dependent meanings."
  },
  {
    "objectID": "ders12-nlp.html#nlp-preprocessing-steps",
    "href": "ders12-nlp.html#nlp-preprocessing-steps",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Preprocessing steps",
    "text": "NLP Preprocessing steps\nText cleaning and normalization:\nThis initial step involves preparing raw text data for analysis by removing or standardizing elements that could interfere with processing. It typically includes tasks like converting text to lowercase to ensure consistency, removing special characters or punctuation that aren’t relevant to the analysis, and handling formatting issues. For instance, in social media text, it might involve expanding contractions (e.g., “don’t” to “do not”), correcting common misspellings, or converting emojis to text descriptions. The goal is to create a standardized, “clean” version of the text that retains its essential meaning while reducing noise and variations that could complicate analysis."
  },
  {
    "objectID": "ders12-nlp.html#section-7",
    "href": "ders12-nlp.html#section-7",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Tokenization\nTokenization is the process of breaking down text into smaller units called tokens, typically words or subwords. While it might seem straightforward, tokenization can be complex due to language-specific challenges. For example, in English, we might split on whitespace and punctuation, but this approach fails for languages like Chinese that don’t use spaces between words. Tokenization also involves decisions about how to handle contractions, hyphenated words, or multi-word expressions. The choice of tokenization method can significantly impact downstream NLP tasks, as it defines the basic units of meaning that the model will work with."
  },
  {
    "objectID": "ders12-nlp.html#section-8",
    "href": "ders12-nlp.html#section-8",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Stop word removal\nStop words are common words (like “the”, “is”, “at”, “which”) that are usually filtered out from text processing because they typically don’t carry significant meaning on their own. Removing them can help reduce noise in the data and focus the analysis on the more meaningful content words. However, the definition of what constitutes a stop word can vary depending on the specific application or domain. For instance, in sentiment analysis, words like “not” that are often considered stop words can be crucial for understanding the meaning. Therefore, while stop word removal can improve efficiency and reduce noise, it requires careful consideration of the specific NLP task at hand."
  },
  {
    "objectID": "ders12-nlp.html#section-9",
    "href": "ders12-nlp.html#section-9",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Stemming and lemmatization\nThese techniques aim to reduce words to their base or dictionary form, which helps in treating different word forms as a single item. Stemming is a crude heuristic process that chops off the ends of words, often resulting in non-words (e.g., “running” becomes “run”, but “universe” becomes “univers”). Lemmatization, on the other hand, uses vocabulary and morphological analysis to return the dictionary form of a word (e.g., “running” becomes “run”, “am” becomes “be”). While stemming is faster and simpler, lemmatization provides more accurate results. The choice between them depends on the specific requirements of the NLP task, balancing between processing speed and linguistic accuracy."
  },
  {
    "objectID": "ders12-nlp.html#section-10",
    "href": "ders12-nlp.html#section-10",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders12-nlp.html#section-11",
    "href": "ders12-nlp.html#section-11",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders12-nlp.html#section-12",
    "href": "ders12-nlp.html#section-12",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Part-of-speech (POS) tagging\nPOS tagging involves labeling each word in a text with its appropriate part of speech (noun, verb, adjective, etc.), based on both its definition and context. This process is crucial for understanding the grammatical structure of sentences and can significantly aid in tasks like named entity recognition or syntactic parsing. Modern POS taggers often use machine learning models trained on large annotated corpora and can achieve high accuracy. However, they can still face challenges with ambiguous words (e.g., “record” can be a noun or a verb) or in handling domain-specific language where words might be used in unconventional ways."
  },
  {
    "objectID": "ders12-nlp.html#section-13",
    "href": "ders12-nlp.html#section-13",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders12-nlp.html#section-14",
    "href": "ders12-nlp.html#section-14",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Part Of Speech\nTag\n\n\n\n\nNoun (Singular)\nNN\n\n\nNoun (Plural)\nNNS\n\n\nVerb\nVB\n\n\nDeterminer\nDT\n\n\nAdjective\nJJ\n\n\nAdverb\nRB"
  },
  {
    "objectID": "ders12-nlp.html#section-15",
    "href": "ders12-nlp.html#section-15",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Syntax analysis and parsing\nThis technique involves analyzing the grammatical structure of sentences to understand how words relate to each other. It typically produces a parse tree or dependency graph representing the sentence structure. There are two main approaches: constituency parsing, which breaks sentences into nested constituents (phrases), and dependency parsing, which identifies grammatical relationships between individual words. Parsing is crucial for tasks that require understanding sentence structure, like machine translation or question answering. However, it can be computationally intensive and faces challenges with ambiguous sentences or non-standard language use.\nPlease visit the following links: link1"
  },
  {
    "objectID": "ders12-nlp.html#feature-extraction-and-representation-techniques-in-nlp",
    "href": "ders12-nlp.html#feature-extraction-and-representation-techniques-in-nlp",
    "title": "NLP - Natural Language Processing",
    "section": "Feature Extraction and Representation Techniques in NLP",
    "text": "Feature Extraction and Representation Techniques in NLP"
  },
  {
    "objectID": "ders12-nlp.html#section-16",
    "href": "ders12-nlp.html#section-16",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Bag of Words (BoW) model:\nThe Bag of Words model is a simple yet effective method for representing text as numerical features. It creates a vocabulary of all unique words in the corpus and represents each document as a vector of word frequencies. The order of words is disregarded, hence the name “bag”. While BoW is computationally efficient and easy to understand, it loses all information about word order and context. It also struggles with out-of-vocabulary words and can result in very high-dimensional, sparse vectors for large vocabularies. Despite these limitations, BoW remains useful for many text classification tasks and serves as a foundation for more advanced techniques."
  },
  {
    "objectID": "ders12-nlp.html#section-17",
    "href": "ders12-nlp.html#section-17",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Term Frequency-Inverse Document Frequency (TF-IDF):\nTF-IDF is a statistical measure used to evaluate the importance of a word in a document within a collection or corpus. It combines two metrics: Term Frequency (how often a word appears in a document) and Inverse Document Frequency (how rare the word is across all documents). The idea is that words frequently occurring in a document but rare in the overall corpus are likely more important for that document. TF-IDF helps to reduce the impact of common words that appear frequently across all documents but carry little meaning. This method provides a more nuanced representation than simple word counts, making it valuable for tasks like information retrieval and document ranking.\nPlease visit this link for demonstration."
  },
  {
    "objectID": "ders12-nlp.html#section-18",
    "href": "ders12-nlp.html#section-18",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "N-gram models:\nN-gram models extend the idea of BoW by considering sequences of N adjacent words, rather than individual words in isolation. For example, a bigram (2-gram) model would consider pairs of adjacent words, while a trigram (3-gram) model would look at sequences of three words. N-grams capture some local context and word order information, which can be crucial for tasks like language modeling or machine translation. However, as N increases, the model’s complexity and data sparsity issues also increase. N-gram models strike a balance between the simplicity of BoW and the need to capture word relationships, making them useful in various NLP applications.\nPlease visit these links for visual explanation: link1 link2 link3"
  },
  {
    "objectID": "ders12-nlp.html#section-19",
    "href": "ders12-nlp.html#section-19",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Word embeddings (Word2Vec, GloVe, FastText):\nWord embeddings are dense vector representations of words in a continuous vector space, where semantically similar words are mapped to nearby points. Unlike BoW or TF-IDF, which produce sparse, high-dimensional vectors, word embeddings typically have a few hundred dimensions. Models like Word2Vec learn these representations by analyzing word co-occurrences in large corpora, capturing semantic relationships (e.g., “king” - “man” + “woman” ≈ “queen”). FastText extends this idea to subword units, allowing it to generate embeddings for out-of-vocabulary words. These dense representations significantly improve performance on various NLP tasks and allow for interesting word algebra. However, they struggle with polysemy (words with multiple meanings) as each word has a single, fixed representation regardless of context.\nPlease visit these links for visual explanations: link1 link2. Please visit Embedding Projector for interactive visualization.\nVisual explanation for FastText"
  },
  {
    "objectID": "ders12-nlp.html#section-20",
    "href": "ders12-nlp.html#section-20",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Contextual embeddings (e.g., BERT, GPT):\nContextual embeddings represent a significant advancement over traditional word embeddings by generating different representations for a word based on its context in a sentence. Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) use deep learning architectures to process entire sentences or documents simultaneously, capturing complex, bidirectional relationships between words. These models are pre-trained on vast amounts of text data and can be fine-tuned for specific tasks. Contextual embeddings excel at capturing nuanced meanings, handling polysemy, and understanding long-range dependencies in text. They have set new state-of-the-art benchmarks on various NLP tasks. However, they are computationally intensive and require significant resources to train and deploy, presenting challenges for applications with limited computational power or real-time requirements."
  },
  {
    "objectID": "ders3_1.html#sample-data",
    "href": "ders3_1.html#sample-data",
    "title": "Data Types and Preprocessing Data",
    "section": "Sample data",
    "text": "Sample data\n\n\n\nNo.\nProfession\nMarital Status\nIncome\nEducation Level\n\n\n\n\n1\nTeacher\nSingle\n8000\nPostgraduate\n\n\n2\nNurse\nSingle\n6000\nBachelor’s\n\n\n3\nWorker\nMarried\n5000\nHigh School\n\n\n4\nWorker\nSingle\n7200\nHigh School\n\n\n5\nPolice\nMarried\n8500\nBachelor’s\n\n\n6\nTeacher\nMarried\n8500\nBachelor’s\n\n\n7\nDoctor\nMarried\n12000\nPostgraduate\n\n\n8\nWorker\nSingle\n5500\nHigh School\n\n\n9\nPolice\nMarried\n8250\nBachelor’s\n\n\n10\nLawyer\nMarried\n12500\nBachelor’s"
  },
  {
    "objectID": "ders3_1.html#data-types",
    "href": "ders3_1.html#data-types",
    "title": "Data Types and Preprocessing Data",
    "section": "Data Types",
    "text": "Data Types\n\nNominal (Categorical) Data: Data type consisting of categories. Not used with ‘more than’ expressions.\n\nBinary (Two-Category) Data: Marital Status {Married, Single}\nMulti-Category Data: Profession {Teacher, Nurse, Worker, Police, Doctor, Lawyer}\n\nOrdinal Data: Data type consisting of categories where categories indicate rank (importance, priority). Can be used with ‘more than’ expressions. Example: Education Level {High School, Bachelor’s, Postgraduate}\nInterval Data: Data type measured on a scale divided into equal parts. Example: Income [5000,12500]\nRatio Data: Data type consisting of continuous values within a certain range. Example: Weight {65.2, 68.1, 73.5, …}"
  },
  {
    "objectID": "ders3_1.html#data-descriptive-characteristics",
    "href": "ders3_1.html#data-descriptive-characteristics",
    "title": "Data Types and Preprocessing Data",
    "section": "Data Descriptive Characteristics",
    "text": "Data Descriptive Characteristics\nPurpose: Better understanding of data.\n\nMeasures of Central Tendency (Arithmetic Mean, Median, Mode)\nMeasures of Dispersion (Variance, Standard Deviation, Quartiles)"
  },
  {
    "objectID": "ders3_1.html#measures-of-central-tendency",
    "href": "ders3_1.html#measures-of-central-tendency",
    "title": "Data Types and Preprocessing Data",
    "section": "Measures of Central Tendency",
    "text": "Measures of Central Tendency\nArithmetic Mean\nArithmetic Mean is defined as\n\\[\\frac{\\sum_{i=1}^n x_i}{n}\\]\nWhere:\n\n\\(x_i\\) are the individual data points.\nn is the total number of data points.\n\nSample data: 5 7 4 6 8 16 11 7\nArithmetic Mean: 8"
  },
  {
    "objectID": "ders3_1.html#median",
    "href": "ders3_1.html#median",
    "title": "Data Types and Preprocessing Data",
    "section": "Median",
    "text": "Median\nThe value that remains in the middle when data is arranged in ascending or descending order.\nSample data: 5 7 4 6 8 16 11 7\nOrdered: 4 5 6 7 7 8 11 16\nMedian: 7 7 -&gt; 7"
  },
  {
    "objectID": "ders3_1.html#mode",
    "href": "ders3_1.html#mode",
    "title": "Data Types and Preprocessing Data",
    "section": "Mode",
    "text": "Mode\nThe most frequently occurring value.\nSample data: 5 7 4 6 8 16 11 7\nIn the sample data, 7 is the mode, occurring twice"
  },
  {
    "objectID": "ders3_1.html#measures-of-dispersion",
    "href": "ders3_1.html#measures-of-dispersion",
    "title": "Data Types and Preprocessing Data",
    "section": "Measures of Dispersion",
    "text": "Measures of Dispersion\nVariance\nVariance measures the spread of a set of data points around their mean. It quantifies how much the values differ from the average (mean). A higher variance indicates greater spread, while a lower variance suggests the data points are closer to the mean.\n\\[ \\sigma^2 = \\frac{\\sum_{i=1}^N (x_i - \\mu)^2}{N} \\]\n\n\\(\\mu\\) : average or mean of the sample"
  },
  {
    "objectID": "ders3_1.html#section",
    "href": "ders3_1.html#section",
    "title": "Data Types and Preprocessing Data",
    "section": "",
    "text": "Standard deviation\nStandard deviation is a measure of how much individual data points deviate, on average, from the mean of the dataset. It provides a sense of the spread of the data in the same units as the original data, making it easier to interpret compared to variance (which is in squared units).\nRelation with Variance:\nThe standard deviation is simply the square root of the variance. Mathematically:\n\\[ std.dev = \\sqrt{\\sigma^2} \\]"
  },
  {
    "objectID": "ders3_1.html#unclean-data",
    "href": "ders3_1.html#unclean-data",
    "title": "Data Types and Preprocessing Data",
    "section": "Unclean data",
    "text": "Unclean data\n\nData collected in real applications:\n\nCan be missing: Some attribute values may not be entered for some objects.\n\nProfession = ’ ’\n\nCan be noisy: May contain errors.\n\nSalary = -10\n\nCan be inconsistent: Attribute values or attribute names may be incompatible.\n\nAge = 35 while Date of Birth = 03/10/2004"
  },
  {
    "objectID": "ders3_1.html#data-preprocessing",
    "href": "ders3_1.html#data-preprocessing",
    "title": "Data Types and Preprocessing Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nData Cleaning: Completing missing attribute values, correcting erroneous data, detecting and cleaning outliers, resolving inconsistencies\nData Integration: Combining data from different data sources\nData Reduction: Excluding data while maintaining same results as original data\nData Transformation: Normalization"
  },
  {
    "objectID": "ders3_1.html#data-cleaning",
    "href": "ders3_1.html#data-cleaning",
    "title": "Data Types and Preprocessing Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nRefers to identifying, correcting, or deleting missing, noisy (erroneous), or inconsistent data.\n\n\nCauses of missing data records:\n\nInability to obtain or unknown attribute value when collecting data\nFailure to recognize necessity of certain attributes during data collection\nHuman, software, or hardware problems\n\n\n\n\n\nIt is the most difficult type of error to identify and correct –&gt;\n\n\nCauses of noisy (erroneous) data records:\n\nFaulty data collection tools\nData entry problems\nData transmission problems\nTechnological limitations\nInconsistency in attribute names\n\n\nCauses of inconsistent data records:\n\nData stored in different data sources\nNon-compliance with functional dependency rules"
  },
  {
    "objectID": "ders3_1.html#data-preprocessing-1",
    "href": "ders3_1.html#data-preprocessing-1",
    "title": "Data Types and Preprocessing Data",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nHow to handle missing data?\n\nExclude data records with missing attribute values\nFill missing attribute values manually\nUse a global variable for missing attribute values (Null, unknown, …)\nFill missing attribute values with the mean value of that attribute\nFill with the average of attribute values from records belonging to the same class\nFill with the most probable attribute values"
  },
  {
    "objectID": "ders3_1.html#data-preprocessing-2",
    "href": "ders3_1.html#data-preprocessing-2",
    "title": "Data Types and Preprocessing Data",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nHow to correct noisy data?\n\nBinning: Data is sorted and divided into equal intervals. Each bin is represented by mean, median, and boundary values.\nRegression: Data is fitted to regression functions.\nClustering: Data is grouped based on similarity. Outliers and extreme values are identified and deleted.\nManual detection of erroneous data: Suspicious values are found and checked by humans."
  },
  {
    "objectID": "ders3_1.html#data-transformation",
    "href": "ders3_1.html#data-transformation",
    "title": "Data Types and Preprocessing Data",
    "section": "Data Transformation",
    "text": "Data Transformation\nCreating new attributes from given attributes.\n\nGeneralization: Summarizing the data.\nNormalization (Statistical Normalization):\n\nUseful when there are significant differences between data, helps bring data into a single format. Enables reducing data to smaller ranges.\nAllows comparison of data from different scaling systems by bringing them into a similar format. The goal here is to transfer data from different systems into a common system and make them comparable using mathematical functions."
  },
  {
    "objectID": "ders3_1.html#types-of-normalization",
    "href": "ders3_1.html#types-of-normalization",
    "title": "Data Types and Preprocessing Data",
    "section": "Types of Normalization",
    "text": "Types of Normalization\n\nZ-Score Normalization\nMin-Max Normalization"
  },
  {
    "objectID": "ders3_1.html#z-score-normalization",
    "href": "ders3_1.html#z-score-normalization",
    "title": "Data Types and Preprocessing Data",
    "section": "Z-score normalization",
    "text": "Z-score normalization\nZ-score normalization (or standardization) transforms data to have a mean of 0 and a standard deviation of 1. This technique is useful for comparing data points from different distributions or preparing data for machine learning algorithms sensitive to scale.\nThe Z-score of a data point xx is calculated as:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nWhere:\n\n\\(x\\): The data point.\n\\(\\mu\\): The mean of the dataset.\n\\(\\sigma\\): The standard deviation of the dataset."
  },
  {
    "objectID": "ders3_1.html#min-max-normalization",
    "href": "ders3_1.html#min-max-normalization",
    "title": "Data Types and Preprocessing Data",
    "section": "Min-max normalization",
    "text": "Min-max normalization\nMin-Max Normalization scales data to a specified range, typically [0,1][0,1], by adjusting the values proportionally within the given range. It is useful for ensuring that all features contribute equally to analyses or machine learning models.\nThe Min-Max Normalization formula is:\n\\[ x' = \\frac{x - x_{min}}{x_{max} - x_{min}} \\]"
  },
  {
    "objectID": "ders3_1.html#section-1",
    "href": "ders3_1.html#section-1",
    "title": "Data Types and Preprocessing Data",
    "section": "",
    "text": "Min-max example: Sample data {10,20,30,40,50}\n\nStep 1: identify min and max, 10 and 50, respectively.\nStep 2: Apply min-max formula to each value\n\n10: (10 - 10)/(50 -10) = 0\n20: (20 - 10)/(50 -10) = 0.25\n30: (30 - 10)/(50 -10) = 0.50\n40: (40 - 10)/(50 -10) = 0.75\n50: (50 - 10)/(50 -10) = 1\n\n\nMin-max normalized data: {0.00, 0.25, 0.50, 0.75, 1.00}"
  },
  {
    "objectID": "ders3_1.html#example",
    "href": "ders3_1.html#example",
    "title": "Data Types and Preprocessing Data",
    "section": "Example",
    "text": "Example\nIn the table below, we have two different attributes with unrelated scales. We applied z-score normaliation to Experience and min-max normaliation to savings data.\nNotice that both features are now between (-0.97, 1.79) and (0, 1)\n\n\n\n\n\n\n\n\n\nExperience (Years)\nSavings (TL)\nzscore_experience\nminmax_savings\n\n\n\n\n6\n100,000\n-0.97\n0.00\n\n\n7\n250,000\n-0.87\n0.23\n\n\n15\n750,000\n-0.08\n1.00\n\n\n15\n150,000\n-0.08\n0.08\n\n\n18\n400,000\n0.21\n0.46\n\n\n34\n650,000\n1.79\n0.85"
  },
  {
    "objectID": "r-linear-regression.html",
    "href": "r-linear-regression.html",
    "title": "Linear Regression with R",
    "section": "",
    "text": "Here we have a data mtcars {datasets} that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). We will find a mathematical model between Weight (wt) and Displacement (disp) parameters. So let’s find the most suitable model. Please draw the plot, determine the degree of equation (first, second of third) and draw the curve.\n\nmtcars\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n [ reached 'max' / getOption(\"max.print\") -- omitted 23 rows ]\n\n\n\nplot(mtcars)\n\n\n\n\n\n\n\n\n\nplot(mtcars$disp, mtcars$wt)\n\n\n\n\n\n\n\n\n\nfirst_wd &lt;- lm(wt ~ disp, data=mtcars)\nsummary(first_wd)\n\n\nCall:\nlm(formula = wt ~ disp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89044 -0.29775 -0.00684  0.33428  0.66525 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.5998146  0.1729964   9.248 2.74e-10 ***\ndisp        0.0070103  0.0006629  10.576 1.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4574 on 30 degrees of freedom\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7815 \nF-statistic: 111.8 on 1 and 30 DF,  p-value: 1.222e-11\n\n\n\nsecond_wd &lt;- lm(wt ~ disp + I(disp^2), data=mtcars)\nsummary(second_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88654 -0.29136 -0.00961  0.32389  0.67010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.631e+00  3.622e-01   4.502 0.000101 ***\ndisp        6.694e-03  3.325e-03   2.013 0.053448 .  \nI(disp^2)   6.206e-07  6.380e-06   0.097 0.923178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4652 on 29 degrees of freedom\nMultiple R-squared:  0.7886,    Adjusted R-squared:  0.774 \nF-statistic: 54.08 on 2 and 29 DF,  p-value: 1.639e-10\n\n\n\nthird_wd &lt;- lm(wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\nsummary(third_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63764 -0.22785  0.00307  0.24614  0.58333 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.111e+00  5.651e-01  -1.966   0.0593 .  \ndisp         4.945e-02  8.198e-03   6.032 1.68e-06 ***\nI(disp^2)   -1.807e-04  3.361e-05  -5.378 9.88e-06 ***\nI(disp^3)    2.243e-07  4.119e-08   5.446 8.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3299 on 28 degrees of freedom\nMultiple R-squared:  0.8973,    Adjusted R-squared:  0.8863 \nF-statistic: 81.57 on 3 and 28 DF,  p-value: 5.959e-14\n\n\n\nplot(mtcars$disp,mtcars$wt)\ncurve(2.243e-07*x^3 -1.807e-04*x^2 +4.945e-02*x -1.111e+00,add=TRUE)\n\n\n\n\n\n\n\n\n\nbest_model &lt;- lm(wt ~ poly(disp,15), data=mtcars)\nsummary(best_model)\n\n\nCall:\nlm(formula = wt ~ poly(disp, 15), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43800 -0.11202 -0.00364  0.16319  0.37783 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.2172500  0.0509709  63.119  &lt; 2e-16 ***\npoly(disp, 15)1   4.8375552  0.2883350  16.778 1.41e-11 ***\npoly(disp, 15)2   0.0452475  0.2883350   0.157   0.8773    \npoly(disp, 15)3   1.7965723  0.2883350   6.231 1.20e-05 ***\npoly(disp, 15)4   0.4463962  0.2883350   1.548   0.1411    \npoly(disp, 15)5  -0.6866393  0.2883350  -2.381   0.0300 *  \npoly(disp, 15)6  -0.3384161  0.2883350  -1.174   0.2577    \npoly(disp, 15)7  -0.6134397  0.2883350  -2.128   0.0493 *  \npoly(disp, 15)8   0.0528245  0.2883350   0.183   0.8569    \npoly(disp, 15)9   0.0814084  0.2883350   0.282   0.7813    \npoly(disp, 15)10 -0.0654665  0.2883350  -0.227   0.8233    \npoly(disp, 15)11  0.2853853  0.2883350   0.990   0.3370    \npoly(disp, 15)12 -0.0007108  0.2883350  -0.002   0.9981    \npoly(disp, 15)13  0.4495723  0.2883350   1.559   0.1385    \npoly(disp, 15)14 -0.0699728  0.2883350  -0.243   0.8113    \npoly(disp, 15)15 -0.5031480  0.2883350  -1.745   0.1002    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2883 on 16 degrees of freedom\nMultiple R-squared:  0.9552,    Adjusted R-squared:  0.9132 \nF-statistic: 22.73 on 15 and 16 DF,  p-value: 5.551e-08\n\n\n\npred &lt;- predict(best_model, data.frame(disp=50:500))\nplot(mtcars$disp, mtcars$wt)\npoints(50:500, pred, type=\"l\")\n\n\n\n\n\n\n\n\n\npredict(third_wd, data.frame(disp=420))\n\n       1 \n4.397246 \n\n\n\npredict(best_model, data.frame(disp=420))\n\n       1 \n7.329183 \n\n\n\npredict(best_model, data.frame(disp=c(20,420, 500)))\n\n           1            2            3 \n 3865.329736     7.329183 -1513.538298 \n\n\n\nSo, the “best” model is not best at all.."
  },
  {
    "objectID": "r-linear-regression.html#a-polynomial-model",
    "href": "r-linear-regression.html#a-polynomial-model",
    "title": "Linear Regression with R",
    "section": "",
    "text": "Here we have a data mtcars {datasets} that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). We will find a mathematical model between Weight (wt) and Displacement (disp) parameters. So let’s find the most suitable model. Please draw the plot, determine the degree of equation (first, second of third) and draw the curve.\n\nmtcars\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n [ reached 'max' / getOption(\"max.print\") -- omitted 23 rows ]\n\n\n\nplot(mtcars)\n\n\n\n\n\n\n\n\n\nplot(mtcars$disp, mtcars$wt)\n\n\n\n\n\n\n\n\n\nfirst_wd &lt;- lm(wt ~ disp, data=mtcars)\nsummary(first_wd)\n\n\nCall:\nlm(formula = wt ~ disp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89044 -0.29775 -0.00684  0.33428  0.66525 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.5998146  0.1729964   9.248 2.74e-10 ***\ndisp        0.0070103  0.0006629  10.576 1.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4574 on 30 degrees of freedom\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7815 \nF-statistic: 111.8 on 1 and 30 DF,  p-value: 1.222e-11\n\n\n\nsecond_wd &lt;- lm(wt ~ disp + I(disp^2), data=mtcars)\nsummary(second_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88654 -0.29136 -0.00961  0.32389  0.67010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.631e+00  3.622e-01   4.502 0.000101 ***\ndisp        6.694e-03  3.325e-03   2.013 0.053448 .  \nI(disp^2)   6.206e-07  6.380e-06   0.097 0.923178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4652 on 29 degrees of freedom\nMultiple R-squared:  0.7886,    Adjusted R-squared:  0.774 \nF-statistic: 54.08 on 2 and 29 DF,  p-value: 1.639e-10\n\n\n\nthird_wd &lt;- lm(wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\nsummary(third_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63764 -0.22785  0.00307  0.24614  0.58333 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.111e+00  5.651e-01  -1.966   0.0593 .  \ndisp         4.945e-02  8.198e-03   6.032 1.68e-06 ***\nI(disp^2)   -1.807e-04  3.361e-05  -5.378 9.88e-06 ***\nI(disp^3)    2.243e-07  4.119e-08   5.446 8.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3299 on 28 degrees of freedom\nMultiple R-squared:  0.8973,    Adjusted R-squared:  0.8863 \nF-statistic: 81.57 on 3 and 28 DF,  p-value: 5.959e-14\n\n\n\nplot(mtcars$disp,mtcars$wt)\ncurve(2.243e-07*x^3 -1.807e-04*x^2 +4.945e-02*x -1.111e+00,add=TRUE)\n\n\n\n\n\n\n\n\n\nbest_model &lt;- lm(wt ~ poly(disp,15), data=mtcars)\nsummary(best_model)\n\n\nCall:\nlm(formula = wt ~ poly(disp, 15), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43800 -0.11202 -0.00364  0.16319  0.37783 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.2172500  0.0509709  63.119  &lt; 2e-16 ***\npoly(disp, 15)1   4.8375552  0.2883350  16.778 1.41e-11 ***\npoly(disp, 15)2   0.0452475  0.2883350   0.157   0.8773    \npoly(disp, 15)3   1.7965723  0.2883350   6.231 1.20e-05 ***\npoly(disp, 15)4   0.4463962  0.2883350   1.548   0.1411    \npoly(disp, 15)5  -0.6866393  0.2883350  -2.381   0.0300 *  \npoly(disp, 15)6  -0.3384161  0.2883350  -1.174   0.2577    \npoly(disp, 15)7  -0.6134397  0.2883350  -2.128   0.0493 *  \npoly(disp, 15)8   0.0528245  0.2883350   0.183   0.8569    \npoly(disp, 15)9   0.0814084  0.2883350   0.282   0.7813    \npoly(disp, 15)10 -0.0654665  0.2883350  -0.227   0.8233    \npoly(disp, 15)11  0.2853853  0.2883350   0.990   0.3370    \npoly(disp, 15)12 -0.0007108  0.2883350  -0.002   0.9981    \npoly(disp, 15)13  0.4495723  0.2883350   1.559   0.1385    \npoly(disp, 15)14 -0.0699728  0.2883350  -0.243   0.8113    \npoly(disp, 15)15 -0.5031480  0.2883350  -1.745   0.1002    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2883 on 16 degrees of freedom\nMultiple R-squared:  0.9552,    Adjusted R-squared:  0.9132 \nF-statistic: 22.73 on 15 and 16 DF,  p-value: 5.551e-08\n\n\n\npred &lt;- predict(best_model, data.frame(disp=50:500))\nplot(mtcars$disp, mtcars$wt)\npoints(50:500, pred, type=\"l\")\n\n\n\n\n\n\n\n\n\npredict(third_wd, data.frame(disp=420))\n\n       1 \n4.397246 \n\n\n\npredict(best_model, data.frame(disp=420))\n\n       1 \n7.329183 \n\n\n\npredict(best_model, data.frame(disp=c(20,420, 500)))\n\n           1            2            3 \n 3865.329736     7.329183 -1513.538298 \n\n\n\nSo, the “best” model is not best at all.."
  },
  {
    "objectID": "r-linear-regression.html#traintest-split",
    "href": "r-linear-regression.html#traintest-split",
    "title": "Linear Regression with R",
    "section": "Train/Test split",
    "text": "Train/Test split\nLet’s understand the topic of overfitting.\nSplit the data into train and test. Train the model with “training” data. Then predict with “test” data.\n\nsample(1:32, 5, replace=FALSE)\n\n[1]  1 11 22 13 30\n\n\n\nset.seed(2)\nidx &lt;- sample(1:32, 5, replace=FALSE)\nidx\n\n[1] 21 15  6 30  8\n\n\n\ntest &lt;- mtcars[idx,]\n\ntest\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona      21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nValiant            18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 240D          24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\n\n\ntrain &lt;- mtcars[-idx,]\ntrain\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n [ reached 'max' / getOption(\"max.print\") -- omitted 18 rows ]\n\n\n\nthird_deg &lt;- lm(wt ~ poly(disp,3), data = train)\nsummary(third_deg)$r.squared\n\n[1] 0.8879368\n\n\n\npredict(third_deg, test)\n\n     Toyota Corona Cadillac Fleetwood            Valiant       Ferrari Dino \n          2.633774           5.824101           3.418857           2.970499 \n         Merc 240D \n          2.989188 \n\n\n\ntest$third_pred &lt;- predict(third_deg, test)\ntest\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona      21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nValiant            18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 240D          24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n                   third_pred\nToyota Corona        2.633774\nCadillac Fleetwood   5.824101\nValiant              3.418857\nFerrari Dino         2.970499\nMerc 240D            2.989188\n\n\n\nfifteen_deg &lt;- lm(wt ~ poly(disp,15), data=train)\nsummary(fifteen_deg)$r.squared\n\n[1] 0.9550231\n\n\n\npredict(fifteen_deg, test)\n\n     Toyota Corona Cadillac Fleetwood            Valiant       Ferrari Dino \n          2.644378        -844.610091           2.784157           2.761685 \n         Merc 240D \n          2.745391 \n\n\n\ntest$fifteen &lt;- predict(fifteen_deg, test)\n\ntest[c(\"disp\",\"wt\",\"third_pred\",\"fifteen\")]\n\n                    disp    wt third_pred     fifteen\nToyota Corona      120.1 2.465   2.633774    2.644378\nCadillac Fleetwood 472.0 5.250   5.824101 -844.610091\nValiant            225.0 3.460   3.418857    2.784157\nFerrari Dino       145.0 2.770   2.970499    2.761685\nMerc 240D          146.7 3.190   2.989188    2.745391\n\n\nAs you can see, “fifteenth degree” model memorized (i.e overfitted) the data and predicts horribly."
  },
  {
    "objectID": "rf-regression-R.html",
    "href": "rf-regression-R.html",
    "title": "Random Forest Regression with R",
    "section": "",
    "text": "# Load required libraries\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n# Set seed for reproducibility\nset.seed(123)\n\ndata &lt;- mtcars\n\n\nSplit data into training and testing sets\n\ntrain_indices &lt;- sample(1:nrow(data), 0.7 * nrow(data))\ntrain_data &lt;- data[train_indices, ]\ntest_data &lt;- data[-train_indices, ]\n\n\n\nTrain Random Forest model and make predictions\n\nrf_model &lt;- randomForest(mpg ~ ., data = train_data, ntree = 500)\n\npredictions &lt;- predict(rf_model, newdata = test_data)\n\n\n\nCalculate RMSE\n\nrmse &lt;- sqrt(mean((test_data$mpg - predictions)^2))\ncat(\"Root Mean Square Error:\", rmse, \"\\n\")\n\nRoot Mean Square Error: 2.00452 \n\n\n\n\nPlot actual vs predicted values\n\nggplot(data.frame(actual = test_data$mpg, predicted = predictions), aes(x = actual, y = predicted)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Actual Values\", y = \"Predicted Values\", title = \"Random Forest Regression: Actual vs Predicted\")\n\n\n\n\n\n\n\n\n\n\nPrint feature importance\n\nimportance &lt;- importance(rf_model)\nprint(importance)\n\n     IncNodePurity\ncyl      152.64821\ndisp     224.33387\nhp       158.42647\ndrat      43.37372\nwt       178.44424\nqsec      34.05620\nvs        25.67666\nam        19.73485\ngear      10.54314\ncarb      21.62506\n\n\n\n\nPlot feature importance\n\nimportance_df &lt;- data.frame(feature = rownames(importance), importance = importance[, 1])\nggplot(importance_df, aes(x = reorder(feature, importance), y = importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(x = \"Features\", y = \"Importance\", title = \"Random Forest: Feature Importance\")\n\n\n\n\n\n\n\n\nalternative at https://hackernoon.com/random-forest-regression-in-r-code-and-interpretation"
  },
  {
    "objectID": "id3-decision-tree.html",
    "href": "id3-decision-tree.html",
    "title": "Decision Tree Algoritm",
    "section": "",
    "text": "Taken from https://medium.datadriveninvestor.com/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38\nEntropy\nIn machine learning, entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information.\n\nInformation Gain\nInformation gain can be defined as the amount of information gained about a random variable or signal from observing another random variable.It can be considered as the difference between the entropy of parent node and weighted average entropy of child nodes.\n\nGini Impurity\nGini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n\nGini impurity is lower bounded by 0, with 0 occurring if the data set contains only one class.\n\nThere are many algorithms there to build a decision tree. They are\n\nCART (Classification and Regression Trees) — This makes use of Gini impurity as the metric.\nID3 (Iterative Dichotomiser 3) — This uses entropy and information gain as metric.\n\nIn this article, I will go through ID3. Once you got it it is easy to implement the same using CART.\n\nClassification using the ID3 algorithm\nConsider whether a dataset based on which we will determine whether to play football or not.\n\nHere There are for independent variables to determine the dependent variable. The independent variables are Outlook, Temperature, Humidity, and Wind. The dependent variable is whether to play football or not.\nAs the first step, we have to find the parent node for our decision tree. For that follow the steps:\n*Find the entropy of the class variable.*\nE(S) = -[(9/14)log(9/14) + (5/14)log(5/14)] = 0.94\nnote: Here typically we will take log to base 2.Here total there are 14 yes/no. Out of which 9 yes and 5 no.Based on it we calculated probability above.\nFrom the above data for outlook we can arrive at the following table easily\n\n*Now we have to calculate average weighted entropy*. ie, we have found the total of weights of each feature multiplied by probabilities.\nE(S, outlook) = (5/14)E(3,2) + (4/14)E(4,0) + (5/14)*E(2,3) = (5/14)(-(3/5)log(3/5)-(2/5)log(2/5))+ (4/14)(0) + (5/14)((2/5)log(2/5)-(3/5)log(3/5)) = 0.693\n*The next step is to find the information gain*. It is the difference between parent entropy and average weighted entropy we found above.\nIG(S, outlook) = 0.94 - 0.693 = 0.247\nSimilarly find Information gain for Temperature, Humidity, and Windy.\nIG(S, Temperature) = 0.940 - 0.911 = 0.029\nIG(S, Humidity) = 0.940 - 0.788 = 0.152\nIG(S, Windy) = 0.940 - 0.8932 = 0.048\n*Now select the feature having the largest entropy gain*. Here it is Outlook. So it forms the first node(root node) of our decision tree.\nNow our data look as follows\n\n\n\nSince overcast contains only examples of class ‘Yes’ we can set it as yes. That means If outlook is overcast football will be played. Now our decision tree looks as follows.\n\nThe next step is to find the next node in our decision tree. Now we will find one under sunny. We have to determine which of the following Temperature, Humidity or Wind has higher information gain.\n\nCalculate parent entropy E(sunny)\nE(sunny) = (-(3/5)log(3/5)-(2/5)log(2/5)) = 0.971.\nNow Calculate the information gain of Temperature. IG(sunny, Temperature)\n\nE(sunny, Temperature) = (2/5)E(0,2) + (2/5)E(1,1) + (1/5)*E(1,0)=2/5=0.4\nNow calculate information gain.\nIG(sunny, Temperature) = 0.971–0.4 =0.571\nSimilarly we get\nIG(sunny, Humidity) = 0.971\nIG(sunny, Windy) = 0.020\nHere IG(sunny, Humidity) is the largest value. So Humidity is the node that comes under sunny.\n\nFor humidity from the above table, we can say that play will occur if humidity is normal and will not occur if it is high. Similarly, find the nodes under rainy.\n*Note: A branch with entropy more than 0 needs further splitting.*\nFinally, our decision tree will look as below:\n\n\n\nClassification using CART algorithm\nClassification using CART is similar to it. But instead of entropy, we use Gini impurity.\nSo as the first step we will find the root node of our decision tree. For that Calculate the Gini index of the class variable\nGini(S) = 1 - [(9/14)² + (5/14)²] = 0.4591\nAs the next step, we will calculate the Gini gain. For that first, we will find the average weighted Gini impurity of Outlook, Temperature, Humidity, and Windy.\nFirst, consider case of Outlook\n\nGini(S, outlook) = (5/14)gini(3,2) + (4/14)gini(4,0)+ (5/14)gini(2,3) = (5/14)(1 - (3/5)² - (2/5)²) + (4/14)*0 + (5/14)(1 - (2/5)² - (3/5)²)= 0.171+0+0.171 = 0.342\nGini gain (S, outlook) = 0.459 - 0.342 = 0.117\nGini gain(S, Temperature) = 0.459 - 0.4405 = 0.0185\nGini gain(S, Humidity) = 0.459 - 0.3674 = 0.0916\nGini gain(S, windy) = 0.459 - 0.4286 = 0.0304\nChoose one that has a higher Gini gain. Gini gain is higher for outlook. So we can choose it as our root node."
  },
  {
    "objectID": "ders11-clustering.html#contents",
    "href": "ders11-clustering.html#contents",
    "title": "Clustering Methods",
    "section": "Contents",
    "text": "Contents\n\n\n\nIntroduction to Clustering\n\nDefinition and purpose, Unsupervised learning concept, Real-world applications in scientific fields\n\nTypes of Clustering Algorithms\n\nPartitioning methods (e.g., K-means)\nHierarchical methods\nDensity-based methods (e.g., DBSCAN)\n\nK-means Clustering\n\nAlgorithm explanation\nChoosing the number of clusters (elbow method)\nAdvantages and limitations\n\n\n\n\nHierarchical Clustering\n\nAgglomerative vs. divisive approaches\nDendrogram interpretation\nLinkage methods (single, complete, average)\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nCore concepts: core points, border points, noise\nAlgorithm explanation\nAdvantages\n\nEvaluation Metrics for Clustering\n\nSilhouette score"
  },
  {
    "objectID": "ders11-clustering.html#definition",
    "href": "ders11-clustering.html#definition",
    "title": "Clustering Methods",
    "section": "Definition",
    "text": "Definition\nClustering is a technique in machine learning and data analysis that involves grouping a set of objects or data points in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups. It’s a fundamental task in many data analysis and pattern recognition problems."
  },
  {
    "objectID": "ders11-clustering.html#purpose",
    "href": "ders11-clustering.html#purpose",
    "title": "Clustering Methods",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of clustering is multifaceted:\nPattern Discovery: Clustering helps in identifying inherent patterns or structures within data that might not be immediately apparent.\nData Summarization: It can be used to compress large datasets by representing many data points with fewer cluster centroids.\nAnomaly Detection: By identifying groups of similar data points, clustering can help in spotting outliers or anomalies that don’t fit into any cluster.\nSegmentation: In fields like marketing, clustering is used to segment customers into groups with similar behaviors or characteristics.\nPreprocessing for Other Algorithms: Clustering can be used as a preprocessing step for other algorithms, reducing the complexity of the data.\nNatural Classification: In scientific fields, clustering can reveal natural groupings in data, such as in biology for grouping similar species or in astronomy for categorizing types of stars."
  },
  {
    "objectID": "ders11-clustering.html#clustering-methods",
    "href": "ders11-clustering.html#clustering-methods",
    "title": "Clustering Methods",
    "section": "Clustering Methods",
    "text": "Clustering Methods\nPartitioning methods: Divide data into non-overlapping subsets, with each data point belonging to exactly one cluster, often optimizing a specific criterion.\nHierarchical Methods: Create a tree-like structure of clusters, either by merging smaller clusters into larger ones or dividing larger clusters into smaller ones.\nDensity-based Methods: Form clusters in areas of high data point density, separated by regions of low density, allowing for discovery of arbitrarily shaped clusters."
  },
  {
    "objectID": "ders11-clustering.html#applications",
    "href": "ders11-clustering.html#applications",
    "title": "Clustering Methods",
    "section": "Applications",
    "text": "Applications\n\nBiology\n\n\nGene Expression Analysis: Grouping genes with similar expression patterns across different conditions or time points.\nSpecies Classification: Clustering organisms based on genetic or morphological features.\nProtein Structure Analysis: Identifying structural motifs in proteins.\n\n\nChemistry\n\n\nMolecular Dynamics: Analyzing conformations of molecules during simulations.\nSpectroscopy: Clustering similar spectra for compound identification.\nDrug Discovery: Grouping chemical compounds with similar properties or effects."
  },
  {
    "objectID": "ders11-clustering.html#section",
    "href": "ders11-clustering.html#section",
    "title": "Clustering Methods",
    "section": "",
    "text": "Physics\n\n\nParticle Physics: Classifying particle collision events in high-energy physics experiments.\nAstrophysics: Clustering stars or galaxies based on their properties (e.g., luminosity, temperature).\nMaterials Science: Analyzing crystal structures and defects in materials."
  },
  {
    "objectID": "ders11-clustering.html#partitioning",
    "href": "ders11-clustering.html#partitioning",
    "title": "Clustering Methods",
    "section": "Partitioning",
    "text": "Partitioning\n\nImage source"
  },
  {
    "objectID": "ders11-clustering.html#k-means-clustering",
    "href": "ders11-clustering.html#k-means-clustering",
    "title": "Clustering Methods",
    "section": "K-means clustering",
    "text": "K-means clustering\nK-means clustering is an unsupervised machine learning algorithm used to group similar data points into clusters. It works by iteratively assigning data points to k predefined clusters based on their similarity to the cluster centers (centroids). The algorithm starts by randomly initializing k centroids in the feature space. It then repeatedly performs two steps:\n\nassign each data point to the nearest centroid, and\nrecalculate the centroids as the mean of all points in each cluster.\n\nThis process continues until the centroids stabilize or a maximum number of iterations is reached. K-means aims to minimize the within-cluster sum of squares, resulting in compact and distinct clusters."
  },
  {
    "objectID": "ders11-clustering.html#k-means-clustering---interactive-demos",
    "href": "ders11-clustering.html#k-means-clustering---interactive-demos",
    "title": "Clustering Methods",
    "section": "K-means clustering - interactive demos",
    "text": "K-means clustering - interactive demos\nPlease visit the following sites:\n\nTwo-dimensional visualization of k-means clustering algorithm\nK-means Clustering - Interactive Demonstration\nVisualizing K-Means Clustering"
  },
  {
    "objectID": "ders11-clustering.html#k-means-clustering-with-r",
    "href": "ders11-clustering.html#k-means-clustering-with-r",
    "title": "Clustering Methods",
    "section": "K-means clustering with R",
    "text": "K-means clustering with R\nPlease visit K-means with R page for the code."
  },
  {
    "objectID": "ders11-clustering.html#advantages-of-k-means-clustering",
    "href": "ders11-clustering.html#advantages-of-k-means-clustering",
    "title": "Clustering Methods",
    "section": "Advantages of K-means clustering",
    "text": "Advantages of K-means clustering\n\nSimplicity: Easy to understand and implement.\nEfficiency: Generally fast and computationally efficient, especially for large datasets.\nScalability: Works well with high-dimensional data.\nFlexibility: Can be adapted to different types of data with appropriate distance metrics.\nInterpretability: Resulting clusters are often easy to interpret."
  },
  {
    "objectID": "ders11-clustering.html#disadvantages-of-k-means-clustering",
    "href": "ders11-clustering.html#disadvantages-of-k-means-clustering",
    "title": "Clustering Methods",
    "section": "Disadvantages of K-means clustering",
    "text": "Disadvantages of K-means clustering\n\nSensitivity to initial centroids: Results can vary based on the initial random placement of centroids.\nRequires predefined k: The number of clusters must be specified beforehand, which isn’t always known.\nAssumes spherical clusters: Performs poorly with non-globular cluster shapes.\nSensitivity to outliers: Outliers can significantly impact centroid calculation and cluster assignment.\nStruggles with varying cluster sizes: Tends to create clusters of similar spatial extent.\nLocal optima: May converge to a local minimum rather than the global optimum.\nInability to handle categorical data directly: Requires preprocessing for non-numerical data."
  },
  {
    "objectID": "ders11-clustering.html#hierarchical-clustering",
    "href": "ders11-clustering.html#hierarchical-clustering",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nHierarchical clustering is an unsupervised machine learning technique used to group similar data points into clusters. Unlike other clustering methods that require a predefined number of clusters, hierarchical clustering creates a tree-like structure of clusters called a dendrogram. This method works by either starting with each data point as its own cluster and iteratively merging the closest clusters (agglomerative approach), or by starting with all data points in one cluster and recursively dividing it (divisive approach). The process continues until a stopping criterion is met, such as a desired number of clusters or a threshold distance between clusters.\nHierarchical clustering is particularly useful for exploring the inherent structure of data at different levels of granularity and for datasets where the number of clusters is not known in advance."
  },
  {
    "objectID": "ders11-clustering.html#how-it-works",
    "href": "ders11-clustering.html#how-it-works",
    "title": "Clustering Methods",
    "section": "How it works",
    "text": "How it works\nHierarchical clustering begins by calculating the distances between all pairs of data points using a chosen distance metric (e.g., Euclidean distance). In the agglomerative approach, which is more common, each data point starts as its own cluster. The algorithm then iteratively merges the two closest clusters based on a linkage criterion (e.g., single linkage, complete linkage, or average linkage). This process repeats, forming larger clusters until all data points are in a single cluster or until the desired number of clusters is reached. The result is a hierarchical structure that can be visualized as a dendrogram, allowing users to choose the level of clustering that best suits their needs.\nPlease visit Hierarchical Clustering in Data Mining and An Introduction to Hierarchical Clustering"
  },
  {
    "objectID": "ders11-clustering.html#section-1",
    "href": "ders11-clustering.html#section-1",
    "title": "Clustering Methods",
    "section": "",
    "text": "Please visit following sites for demos\n\nClustering Demo at yFiles"
  },
  {
    "objectID": "ders11-clustering.html#advantages-of-hierarchical-clustering",
    "href": "ders11-clustering.html#advantages-of-hierarchical-clustering",
    "title": "Clustering Methods",
    "section": "Advantages of hierarchical clustering",
    "text": "Advantages of hierarchical clustering\n\nNo predefined number of clusters required: Unlike k-means, hierarchical clustering doesn’t require specifying the number of clusters in advance.\nHierarchical representation: Provides a dendrogram, offering insights into the data structure at different levels.\nFlexibility in cluster granularity: Users can choose the level of clustering that best suits their needs by cutting the dendrogram at different levels.\nApplicability to various data types: Can be applied to any data type for which a similarity or distance measure can be defined.\nInterpretability: The dendrogram provides a visually intuitive representation of the clustering process.\nHandles different cluster shapes: Can identify clusters of various shapes and sizes, not just spherical ones."
  },
  {
    "objectID": "ders11-clustering.html#disadvantages",
    "href": "ders11-clustering.html#disadvantages",
    "title": "Clustering Methods",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nComputational complexity: O(n^2) space complexity and O(n^3) time complexity in many implementations, making it less suitable for very large datasets.\nSensitivity to noise and outliers: Outliers can significantly affect the shape of the dendrogram.\nDifficulty in handling high-dimensional data: Performance and interpretability may decrease as the number of dimensions increases.\nLack of backtracking: Once a merge or split is made, it can’t be undone, which may lead to suboptimal results.\nMemory intensive: Storing the distance matrix for large datasets can be memory-intensive.\nChoice of distance metric and linkage method: Results can vary significantly based on the chosen distance metric and linkage method, requiring careful consideration.\nScalability issues: Not as scalable as some other clustering methods for very large datasets."
  },
  {
    "objectID": "ders11-clustering.html#dbscan-clustering",
    "href": "ders11-clustering.html#dbscan-clustering",
    "title": "Clustering Methods",
    "section": "DBSCAN clustering",
    "text": "DBSCAN clustering\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm that groups together points that are closely packed in space, marking points that lie alone in low-density regions as outliers.\nIt works by selecting a random unvisited point and exploring its neighborhood within a specified radius (ε). If this neighborhood contains a minimum number of points (MinPts), a cluster is formed. The algorithm then iteratively expands the cluster by exploring the neighborhoods of the newly added points. If a point doesn’t have enough neighbors to form a cluster, it’s labeled as noise. This process continues until all points have been visited, resulting in a set of dense clusters of varying shapes and sizes, along with identified noise points."
  },
  {
    "objectID": "ders11-clustering.html#interactive-demos",
    "href": "ders11-clustering.html#interactive-demos",
    "title": "Clustering Methods",
    "section": "Interactive demos",
    "text": "Interactive demos\nPlease visit DBSCAN— A visualized and detailed introduction page about core concepts\nPlease visit Visualizing DBSCAN Clustering page for interactive demo for DBSCAN algoritm"
  },
  {
    "objectID": "ders11-clustering.html#dbscan-algoritm",
    "href": "ders11-clustering.html#dbscan-algoritm",
    "title": "Clustering Methods",
    "section": "DBSCAN Algoritm",
    "text": "DBSCAN Algoritm"
  },
  {
    "objectID": "ders11-clustering.html#advantages-of-dbscan",
    "href": "ders11-clustering.html#advantages-of-dbscan",
    "title": "Clustering Methods",
    "section": "Advantages of DBSCAN",
    "text": "Advantages of DBSCAN\n\nShape flexibility: Can find arbitrarily shaped clusters, not just circular ones.\nOutlier detection: Automatically identifies noise points and outliers.\nNo pre-defined number of clusters: Unlike k-means, doesn’t require specifying the number of clusters beforehand.\nDensity-based: Effective at separating clusters of different densities.\nRobustness: Less sensitive to outliers compared to centroid-based algorithms."
  },
  {
    "objectID": "ders11-clustering.html#disadvantages-1",
    "href": "ders11-clustering.html#disadvantages-1",
    "title": "Clustering Methods",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nParameter sensitivity: Results can vary significantly based on the choice of ε and MinPts parameters.\nStruggle with varying densities: May have difficulty when clusters have widely different densities.\nHigh-dimensional data: Performance and effectiveness can decrease in high-dimensional spaces due to the “curse of dimensionality”.\nMemory requirements: Can be memory-intensive for large datasets.\nScalability: Not as efficient for very large datasets compared to some other algorithms.\nConnectivity-based: May struggle with datasets where clusters are close but separated by sparse areas."
  },
  {
    "objectID": "ders11-clustering.html#evaluating-clustering",
    "href": "ders11-clustering.html#evaluating-clustering",
    "title": "Clustering Methods",
    "section": "Evaluating clustering",
    "text": "Evaluating clustering\nDefinition: The silhouette score ranges from -1 to 1, where:\n\nA score close to 1 indicates that the data point is well-matched to its own cluster and poorly-matched to neighboring clusters.\nA score around 0 suggests that the data point is on or very close to the decision boundary between two neighboring clusters.\nA negative score indicates that the data point might have been assigned to the wrong cluster.\n\nCalculation: For each data point i, the silhouette score s(i) is calculated as:\ns(i) = (b(i) - a(i)) / max(a(i), b(i))\nWhere: a(i) is the average distance between i and all other points in the same clusteri, (i) is the average distance between i and all points in the nearest cluster that i is not a part of\nInterpretation:\n\nHigher silhouette scores indicate better-defined clusters.\nThe average silhouette score across all points can be used to compare different clustering configurations."
  },
  {
    "objectID": "ders11-clustering.html#classification-vs.-clustering",
    "href": "ders11-clustering.html#classification-vs.-clustering",
    "title": "Clustering Methods",
    "section": "Classification vs. Clustering",
    "text": "Classification vs. Clustering\nplease compare these two concepts"
  },
  {
    "objectID": "ders8-classification.html#contents",
    "href": "ders8-classification.html#contents",
    "title": "Classification Methods",
    "section": "Contents",
    "text": "Contents\n\n\n\nIntroduction to supervised learning\nDefinition and applications of classification\nPreparing the data\n\nFeature selection and preprocessing ✅\n\nMethods\n\nDecision Trees\nRandom Forest\nSupport Vector Machines (SVM)\nLogistic Regression\nK-nearest neighbor\nNaive Bayes\nArtificial Neural Networks ✅\n\n\n\n\nEnsemble methods\nEvaluation ✅\n\nConfusion Matrix ✅\nAccuracy, precision, recall, F1-score ✅\nROC curves ✅\n\nOverfitting and underfitting ✅\nCross-validation"
  },
  {
    "objectID": "ders8-classification.html#ai---ml---dl",
    "href": "ders8-classification.html#ai---ml---dl",
    "title": "Classification Methods",
    "section": "AI - ML - DL",
    "text": "AI - ML - DL"
  },
  {
    "objectID": "ders8-classification.html#ml-types",
    "href": "ders8-classification.html#ml-types",
    "title": "Classification Methods",
    "section": "ML types",
    "text": "ML types\n\nClassical ML\n\nSupervised\nUnsupervised\n\nReinforcement\nArtificial Neural Networks and Deep Learning"
  },
  {
    "objectID": "ders8-classification.html#ml-types-1",
    "href": "ders8-classification.html#ml-types-1",
    "title": "Classification Methods",
    "section": "ML types",
    "text": "ML types\n\nClassical ML\n\nSupervised\n\nClassification\nRegression\n\nUnsupervised\n\nClustering"
  },
  {
    "objectID": "ders8-classification.html#section",
    "href": "ders8-classification.html#section",
    "title": "Classification Methods",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders8-classification.html#section-1",
    "href": "ders8-classification.html#section-1",
    "title": "Classification Methods",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders8-classification.html#definition-of-classification",
    "href": "ders8-classification.html#definition-of-classification",
    "title": "Classification Methods",
    "section": "Definition of classification",
    "text": "Definition of classification\n\nA type of supervised learning\nGoal: Categorize input data into predefined classes or categories\nThe model learns to draw decision boundaries between classes\nOutput is a discrete class label (unlike regression, which predicts continuous values)"
  },
  {
    "objectID": "ders8-classification.html#applications",
    "href": "ders8-classification.html#applications",
    "title": "Classification Methods",
    "section": "Applications",
    "text": "Applications\n\n\n\nText Classification\n\nSpam detection in emails\nSentiment analysis of product reviews\nNews article categorization\n\nImage Classification\n\nMedical imaging for disease detection\nFacial recognition systems\nPlant or animal species identification\n\nFinancial Applications\n\nCredit scoring (approve/deny loan applications)\nFraud detection in transactions\n\n\n\n\nHealthcare\n\nDisease diagnosis based on symptoms and test results\nPredicting patient readmission risk\n\nEnvironmental Science\n\nClimate pattern classification\nSpecies habitat prediction\n\nLiterature and Linguistics\n\nAuthorship attribution\nGenre classification of texts\nLanguage identification"
  },
  {
    "objectID": "ders8-classification.html#your-turn",
    "href": "ders8-classification.html#your-turn",
    "title": "Classification Methods",
    "section": "Your Turn",
    "text": "Your Turn\nIn the zoom chat window please write down your department and an example of classification task related to your domain"
  },
  {
    "objectID": "ders8-classification.html#your-turn-1",
    "href": "ders8-classification.html#your-turn-1",
    "title": "Classification Methods",
    "section": "Your Turn",
    "text": "Your Turn\nPick one example and discuss about the data\nVisit Kaggle and find related dataset"
  },
  {
    "objectID": "ders8-classification.html#training",
    "href": "ders8-classification.html#training",
    "title": "Classification Methods",
    "section": "Training",
    "text": "Training\n\nHold out\nCross validation"
  },
  {
    "objectID": "ders8-classification.html#hold-out",
    "href": "ders8-classification.html#hold-out",
    "title": "Classification Methods",
    "section": "Hold-out",
    "text": "Hold-out\n\n\nimage source"
  },
  {
    "objectID": "ders8-classification.html#cross-validation",
    "href": "ders8-classification.html#cross-validation",
    "title": "Classification Methods",
    "section": "Cross validation",
    "text": "Cross validation\n\n\nimage source"
  },
  {
    "objectID": "ders8-classification.html#training-1",
    "href": "ders8-classification.html#training-1",
    "title": "Classification Methods",
    "section": "Training",
    "text": "Training\nIn case of Imbalance - Down sampling - Up sampling"
  },
  {
    "objectID": "ders8-classification.html#ml-flow",
    "href": "ders8-classification.html#ml-flow",
    "title": "Classification Methods",
    "section": "ML Flow",
    "text": "ML Flow"
  },
  {
    "objectID": "ders8-classification.html#decision-trees",
    "href": "ders8-classification.html#decision-trees",
    "title": "Classification Methods",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision Trees are a classification method that uses a tree-like model of decisions and their possible consequences. The algorithm learns a series of if-then-else decision rules that split the data based on feature values, creating a structure that resembles a flowchart. Each internal node represents a “test” on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or decision.\nbranch, test, leaf"
  },
  {
    "objectID": "ders8-classification.html#example-data",
    "href": "ders8-classification.html#example-data",
    "title": "Classification Methods",
    "section": "Example Data",
    "text": "Example Data\n\n\nimage source"
  },
  {
    "objectID": "ders8-classification.html#resulting-decision-tree",
    "href": "ders8-classification.html#resulting-decision-tree",
    "title": "Classification Methods",
    "section": "Resulting decision tree",
    "text": "Resulting decision tree\n\n\nimage source"
  },
  {
    "objectID": "ders8-classification.html#example",
    "href": "ders8-classification.html#example",
    "title": "Classification Methods",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\n\n\nHours studied\nPrevious Score\nAttended Review\nPass?\n\n\n\n\n3\n60\nNo\n?\n\n\n4\n75\nNo\n?\n\n\n7\n80\nYes\n?"
  },
  {
    "objectID": "ders8-classification.html#key-metrics-for-decision-tree-construction",
    "href": "ders8-classification.html#key-metrics-for-decision-tree-construction",
    "title": "Classification Methods",
    "section": "Key Metrics for Decision Tree Construction",
    "text": "Key Metrics for Decision Tree Construction\nQuestions: Which feature is the first branch? At what value we create a branch (5 hours, 70 points, etc.)\n\n\nEntropy\nInformation Gain\nGini Impurity"
  },
  {
    "objectID": "ders8-classification.html#entropy",
    "href": "ders8-classification.html#entropy",
    "title": "Classification Methods",
    "section": "Entropy",
    "text": "Entropy\n\nEntropy is a measure of impurity or uncertainty in a set of examples. In the context of decision trees, it quantifies the disorder in the class labels of a dataset.\n\nFormula: \\(H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)\nWhere \\(S\\) is the dataset, \\(c\\) is the number of classes, and \\(p_i\\) is the proportion of examples belonging to class \\(i\\).\n\nRanges from 0 (completely pure, all examples belong to one class) to \\(\\log_2(c)\\) (completely impure, equal distribution across all classes).\nUsed to calculate information gain."
  },
  {
    "objectID": "ders8-classification.html#information-gain",
    "href": "ders8-classification.html#information-gain",
    "title": "Classification Methods",
    "section": "Information Gain",
    "text": "Information Gain\n\nInformation gain measures the reduction in entropy achieved by splitting the data on a particular feature. It helps determine which feature to split on at each node of the decision tree.\n\nFormula: \\(IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\\)\nWhere \\(S\\) is the dataset, \\(A\\) is the feature being considered for splitting, \\(Values(A)\\) are the possible values of feature \\(A\\), and \\(S_v\\) is the subset of \\(S\\) where feature \\(A\\) has value \\(v\\).\n\nHigher information gain indicates a more useful feature for classification.\nThe feature with the highest information gain is typically chosen for splitting at each node."
  },
  {
    "objectID": "ders8-classification.html#gini-impurity",
    "href": "ders8-classification.html#gini-impurity",
    "title": "Classification Methods",
    "section": "Gini Impurity",
    "text": "Gini Impurity\n\nGini impurity is an alternative to entropy for measuring the impurity of a set of examples. It represents the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the subset.\n\nFormula: \\(Gini(S) = 1 - \\sum_{i=1}^{c} (p_i)^2\\)\nWhere \\(S\\) is the dataset, \\(c\\) is the number of classes, and \\(p_i\\) is the proportion of examples belonging to class \\(i\\).\n\nRanges from 0 (completely pure) to \\(1 - \\frac{1}{c}\\) (completely impure).\nOften used in algorithms like CART (Classification and Regression Trees).\n\nThe choice between using entropy (with information gain) or Gini impurity often depends on the specific implementation of the decision tree algorithm. In practice, they often yield similar results."
  },
  {
    "objectID": "ders8-classification.html#algoritms-for-decision-tree-construction",
    "href": "ders8-classification.html#algoritms-for-decision-tree-construction",
    "title": "Classification Methods",
    "section": "Algoritms for decision tree construction",
    "text": "Algoritms for decision tree construction\n\nID3\nCART\n\nPlease visit this link for details about algoritms"
  },
  {
    "objectID": "ders8-classification.html#an-example-with-r",
    "href": "ders8-classification.html#an-example-with-r",
    "title": "Classification Methods",
    "section": "An example with R",
    "text": "An example with R\nhttps://www.dataspoof.info/post/decision-tree-classification-in-r/\nhttps://forum.posit.co/t/decision-tree-in-r/5561/5"
  },
  {
    "objectID": "ders8-classification.html#section-2",
    "href": "ders8-classification.html#section-2",
    "title": "Classification Methods",
    "section": "",
    "text": "Advantages of Decision Trees:\n\nInterpretability: Easy to understand and explain, even for non-experts. The decision-making process can be visually represented.\nNo or little data preprocessing required: Can handle both numerical and categorical data without the need for normalization or scaling.\nComputationally efficient: Generally fast to train and make predictions, especially with small to medium-sized datasets.\n\nDisadvantages:\n\nOverfitting: Prone to overfitting, especially with deep trees, leading to poor generalization on new data.\nInstability: Small changes in the data can result in a completely different tree being generated.\nDifficulty with high-dimensional data: Can become computationally expensive and prone to overfitting with many features."
  },
  {
    "objectID": "ders8-classification.html#quiz-time",
    "href": "ders8-classification.html#quiz-time",
    "title": "Classification Methods",
    "section": "Quiz time",
    "text": "Quiz time"
  },
  {
    "objectID": "ders8-classification.html#random-forest",
    "href": "ders8-classification.html#random-forest",
    "title": "Classification Methods",
    "section": "Random Forest",
    "text": "Random Forest\nRandom forest is a commonly-used machine learning algorithm, trademarked by Leo Breiman and Adele Cutler, that combines the output of multiple decision trees to reach a single result.\nRandom forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees."
  },
  {
    "objectID": "ders8-classification.html#kaggle-example",
    "href": "ders8-classification.html#kaggle-example",
    "title": "Classification Methods",
    "section": "Kaggle example",
    "text": "Kaggle example\nPlease visit: https://www.kaggle.com/code/lara311/diabetes-prediction-using-machine-learning"
  },
  {
    "objectID": "ders8-classification.html#support-vector-machines",
    "href": "ders8-classification.html#support-vector-machines",
    "title": "Classification Methods",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nThe Basic Idea\nImagine you’re trying to separate different types of objects, like apples and oranges, based on their characteristics, such as color, shape, and size. You want to find a way to draw a line (or a hyperplane in higher dimensions) that separates the two types of objects as accurately as possible."
  },
  {
    "objectID": "ders8-classification.html#the-svm-method",
    "href": "ders8-classification.html#the-svm-method",
    "title": "Classification Methods",
    "section": "The SVM Method",
    "text": "The SVM Method\nA Support Vector Machine is a type of supervised learning algorithm that aims to find the best hyperplane that separates the data into different classes. Here’s how it works:\n\nData Preparation: Collect a dataset of objects (e.g., apples and oranges) with their corresponding characteristics (features) and labels (e.g., “apple” or “orange”).\nPlotting the Data: Plot the data points in a feature space, where each axis represents a feature (e.g., color, shape, size).\nFinding the Hyperplane: The goal is to find a hyperplane that separates the data points into different classes. A hyperplane is a line (in 2D) or a plane (in 3D) that divides the feature space into two regions.\nMaximizing the Margin: The SVM algorithm tries to find the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the nearest data points (called support vectors) on either side of the hyperplane.\nSupport Vectors: The support vectors are the data points that lie closest to the hyperplane and have the most influence on its position. They are the “support” that helps define the hyperplane."
  },
  {
    "objectID": "ders8-classification.html#support-vector-machines-1",
    "href": "ders8-classification.html#support-vector-machines-1",
    "title": "Classification Methods",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nKey Concepts\n\nHyperplane: A line (in 2D) or a plane (in 3D) that separates the data into different classes.\nMargin: The distance between the hyperplane and the nearest data points (support vectors) on either side of the hyperplane.\nSupport Vectors: The data points that lie closest to the hyperplane and have the most influence on its position."
  },
  {
    "objectID": "ders8-classification.html#why-svms-are-useful",
    "href": "ders8-classification.html#why-svms-are-useful",
    "title": "Classification Methods",
    "section": "Why SVMs are Useful",
    "text": "Why SVMs are Useful\nSVMs are powerful because they:\n\nCan handle high-dimensional data\nAre robust to noise and outliers\nCan be used for both classification and regression tasks\nProvide a clear geometric interpretation of the decision boundary"
  },
  {
    "objectID": "ders8-classification.html#section-3",
    "href": "ders8-classification.html#section-3",
    "title": "Classification Methods",
    "section": "",
    "text": "H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximal margin. Source"
  },
  {
    "objectID": "ders8-classification.html#section-4",
    "href": "ders8-classification.html#section-4",
    "title": "Classification Methods",
    "section": "",
    "text": "Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. Source"
  },
  {
    "objectID": "ders8-classification.html#section-5",
    "href": "ders8-classification.html#section-5",
    "title": "Classification Methods",
    "section": "",
    "text": "Please visit SVM demo site for an online interactive demo for SVM"
  },
  {
    "objectID": "ders8-classification.html#logistic-regression",
    "href": "ders8-classification.html#logistic-regression",
    "title": "Classification Methods",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe Basic Idea\nLogistic regression is a supervised machine learning algorithm that accomplishes binary classification tasks by predicting the probability of an outcome, event, or observation. The model delivers a binary outcome limited to two possible outcomes: yes/no, 0/1, or true/false."
  },
  {
    "objectID": "ders8-classification.html#the-logistic-regression-method",
    "href": "ders8-classification.html#the-logistic-regression-method",
    "title": "Classification Methods",
    "section": "The Logistic Regression Method",
    "text": "The Logistic Regression Method\nLogistic Regression is a type of supervised learning algorithm that models the probability of an event occurring (e.g., passing an exam) based on a set of input variables (e.g., scores). Here’s how it works:\n\nData Preparation: Collect a dataset of input variables (e.g., scores) and output variables (e.g., pass/fail).\nLogistic Function: The logistic function, also known as the sigmoid function, is used to model the probability of the event occurring. It maps the input variables to a probability between 0 and 1.\nLog-Odds: The logistic function is based on the log-odds of the event occurring, which is the logarithm of the ratio of the probability of the event occurring to the probability of the event not occurring.\nCoefficients: The algorithm learns the coefficients (weights) for each input variable, which determine the importance of each variable in predicting the output.\nDecision Boundary: The algorithm uses the coefficients and the logistic function to create a decision boundary, which separates the input space into two regions: one for each class (e.g., pass and fail).\nPrediction: For a new input, the algorithm calculates the probability of the event occurring using the logistic function and the learned coefficients. If the probability is above a certain threshold (e.g., 0.5), the algorithm predicts the event will occur (e.g., the student will pass)."
  },
  {
    "objectID": "ders8-classification.html#key-concepts",
    "href": "ders8-classification.html#key-concepts",
    "title": "Classification Methods",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLogistic Function: A mathematical function that maps input variables to a probability between 0 and 1.\nLog-Odds: The logarithm of the ratio of the probability of the event occurring to the probability of the event not occurring.\nCoefficients: The weights learned by the algorithm for each input variable, which determine their importance in predicting the output.\nDecision Boundary: The boundary that separates the input space into two regions, one for each class."
  },
  {
    "objectID": "ders8-classification.html#why-logistic-regression-is-useful",
    "href": "ders8-classification.html#why-logistic-regression-is-useful",
    "title": "Classification Methods",
    "section": "Why Logistic Regression is Useful",
    "text": "Why Logistic Regression is Useful\nLogistic Regression is a popular algorithm because it:\n\nIs easy to implement and interpret\nCan handle multiple input variables\nProvides a probability estimate for each prediction\nIs widely used in many fields, such as medicine, finance, and marketing"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "FEF3001 Yapay Zekaya Giriş Ders Notları",
    "section": "",
    "text": "YTÜ Fen Fakültesinde 2024 Güz döneminde verilen FEF3001 kodlu dersin içerikleri sayfasıdır.\n\n\n\nVize yüz yüze mi? Cevap: Evet\nVize konuları nelerdir? Cevap: Yapay sinir ağı konusunun sonuna kadar\nYüklenen sunumlar dışında bir kaynaktan soru sorulacak mıdır? Cevap: Temel olarak sunumlar kullanılacaktır fakat derste anlatılanlardan da sorumluyuz, ders video kaydı ve derste aldığınız notlarınızı da çalışınız\nVize test şeklinde mi ve kaç soru sorulur? Cevap: Test şeklinde 20-30 soru planlanmaktadır\nYorum soruları sorulacak mı? Cevap: Genelde bilgi ölçen sorular tercih edilecektir"
  },
  {
    "objectID": "about.html#sıkça-sorulan-sorular",
    "href": "about.html#sıkça-sorulan-sorular",
    "title": "FEF3001 Yapay Zekaya Giriş Ders Notları",
    "section": "",
    "text": "Vize yüz yüze mi? Cevap: Evet\nVize konuları nelerdir? Cevap: Yapay sinir ağı konusunun sonuna kadar\nYüklenen sunumlar dışında bir kaynaktan soru sorulacak mıdır? Cevap: Temel olarak sunumlar kullanılacaktır fakat derste anlatılanlardan da sorumluyuz, ders video kaydı ve derste aldığınız notlarınızı da çalışınız\nVize test şeklinde mi ve kaç soru sorulur? Cevap: Test şeklinde 20-30 soru planlanmaktadır\nYorum soruları sorulacak mı? Cevap: Genelde bilgi ölçen sorular tercih edilecektir"
  }
]